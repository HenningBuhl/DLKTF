{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TCN_Pytorch.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YuuQk_InZVgy","colab_type":"text"},"source":["# TCN Definition"]},{"cell_type":"code","metadata":{"id":"g-yqzB19GxRo","colab_type":"code","colab":{}},"source":["# Used as argument container.\n","class ArgsObject(object):\n","    pass"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wRKSqymxr8cq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":129},"outputId":"831263c1-f37c-4820-dd2b-f9dd3bd5bf43","executionInfo":{"status":"ok","timestamp":1557148675094,"user_tz":-120,"elapsed":22692,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}}},"source":["# Make data in Google Drive available to this notebook.\n","from google.colab import drive\n","base_dir = '/content/drive/My Drive/Colab Notebooks/TCN/'\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SBFvHMwIZWRC","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","from torch.nn.utils import weight_norm"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnLqvLIiaRoy","colab_type":"text"},"source":["![alt text](https://previews.dropbox.com/p/thumb/AAZsV4RT9icmX7RRdW2bQhMc-Nx1kIMGkxnwwmBycTHXFwfdG-iMAQTxMvuNqr6GSuP5KewsBWNjiWoctGeQVhVQym374IXitho6p305757mMf-FX6BAFlK2orz5tWx-mHi6Gbw-aWpAOckuJvdcEHSPnjMGGYTLTiTDDGTBWFh14Tau6KsGDjw5rARvASxJaCTkn-cHUn0jRjeKDZbLJK447Tmf-yEAHuDzdZ5UpJXBOlg_b9AIfMCjW86MPAg8cebUO5GyvF3QS2ECMUD7A6EJMPdwHnLiBv0ve39AnkCw_x50bsfI7G0NNsuAHhen6T2ylsrFpbQ7oBZCeOxWq0hc/p.png?size_mode=5)"]},{"cell_type":"code","metadata":{"id":"G1vTvGS8sbw0","colab_type":"code","colab":{}},"source":["class Chomp1d(nn.Module):\n","    def __init__(self, chomp_size):\n","        super(Chomp1d, self).__init__()\n","        self.chomp_size = chomp_size\n","    \n","    def forward(self, x):\n","        return x[:, :, :-self.chomp_size].contiguous()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mBwldPR0scLg","colab_type":"code","colab":{}},"source":["class TemporalBlock(nn.Module):\n","    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n","        super(TemporalBlock, self).__init__()\n","        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n","                                           stride=stride, padding=padding, dilation=dilation))\n","        self.chomp1 = Chomp1d(padding)\n","        self.relu1 = nn.ReLU()\n","        self.dropout1 = nn.Dropout(dropout)\n","        \n","        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n","                                           stride=stride, padding=padding, dilation=dilation))\n","        self.chomp2 = Chomp1d(padding)\n","        self.relu2 = nn.ReLU()\n","        self.dropout2 = nn.Dropout(dropout)\n","        \n","        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n","                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n","        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n","        self.relu = nn.ReLU()\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.conv1.weight.data.normal_(0, 0.01)\n","        self.conv2.weight.data.normal_(0, 0.01)\n","        if self.downsample is not None:\n","            self.downsample.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        out = self.net(x)\n","        res = x if self.downsample is None else self.downsample(x)\n","        return self.relu(out + res)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V42HpsjXscUj","colab_type":"code","colab":{}},"source":["class TemporalConvNet(nn.Module):\n","    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n","        super(TemporalConvNet, self).__init__()\n","        layers = []\n","        num_levels = len(num_channels)\n","        for i in range(num_levels):\n","            dilation_size = 2 ** i\n","            in_channels = num_inputs if i == 0 else num_channels[i-1]\n","            out_channels = num_channels[i]\n","            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n","                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n","        self.network = nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        return self.network(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DqOYe1DzZhmg","colab_type":"text"},"source":["# Adding Problem"]},{"cell_type":"markdown","metadata":{"id":"slTe-DHL6bc4","colab_type":"text"},"source":["In this task, each input consists of a length-T sequence of depth 2, with all values randomly\n","chosen randomly in [0, 1] in dimension 1. The second dimension consists of all zeros except for\n","two elements, which are marked by 1. The objective is to sum the two random values whose second \n","dimensions are marked by 1. One can think of this as computing the dot product of two dimensions.\n","\n","Simply predicting the sum to be 1 should give an MSE of about 0.1767. "]},{"cell_type":"markdown","metadata":{"id":"BnrgK5d2HzJF","colab_type":"text"},"source":["![alt text](https://previews.dropbox.com/p/thumb/AAZkFu1jX9YUOBDp9B5XFGbB8QZOS24MJCNJ89qgIU2DBS_xWpUBtGPf3Gh7egBGz-qgj7u4u-FJe4jMM9xl2d2eglf4YXo8Cc1CA-NpEerfjW7qxYjT-PB29S0CRS0cTmzQgyhfsW-j_Cw4HxETkhTXngRdcvSzTB5jYpLOZzACyoPVALjF0z0hP4LEqlzTeeZrYMMZMsU5lZ-1sIKjWKRYjqEk-jIxYwrdjAln-eZ3Zq-pTtFfAKNWsQu1x8Fj2NBoO_nX5zuLM9RQYBPa7alcWchMrVGOlIvCA4RMkUQpyWLFCRrhj6beFUma-57wPgniz_g65cIsbgS8gHfvUV_B/p.png?size=2048x1536&size_mode=3)"]},{"cell_type":"markdown","metadata":{"id":"5pbQfTej6R58","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"sjDxwDR46SXy","colab_type":"code","colab":{}},"source":["import torch\n","import numpy as np\n","from torch.autograd import Variable\n","\n","\n","def data_generator(N, seq_length):\n","    \"\"\"\n","    Args:\n","        seq_length: Length of the adding problem data\n","        N: # of data in the set\n","    \"\"\"\n","    X_num = torch.rand([N, 1, seq_length])\n","    X_mask = torch.zeros([N, 1, seq_length])\n","    Y = torch.zeros([N, 1])\n","    for i in range(N):\n","        positions = np.random.choice(seq_length, size=2, replace=False)\n","        X_mask[i, 0, positions[0]] = 1\n","        X_mask[i, 0, positions[1]] = 1\n","        Y[i,0] = X_num[i, 0, positions[0]] + X_num[i, 0, positions[1]]\n","    X = torch.cat((X_num, X_mask), dim=1)\n","    return Variable(X), Variable(Y)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xe8I3BEQZt8v","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"fJRAFYZcsccq","colab_type":"code","colab":{}},"source":["from torch import nn\n","\n","\n","class TCN(nn.Module):\n","    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n","        super(TCN, self).__init__()\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n","        self.linear = nn.Linear(num_channels[-1], output_size)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.linear.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        y1 = self.tcn(x)\n","        return self.linear(y1[:, :, -1])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OK3ae56W5Zs_","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"JOp80aYF5aNV","colab_type":"code","outputId":"4ebc0ff8-2fc2-40a4-a3f5-4e7411b205f1","executionInfo":{"status":"ok","timestamp":1557154849409,"user_tz":-120,"elapsed":226938,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":3360}},"source":["import torch\n","import argparse\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.batch_size = 32 # batch size (default: 32)\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.0 # dropout applied to layers (default: 0.0)\n","args.clip = -1 # gradient clip, -1 means no clip (default: -1)\n","args.epochs = 10 # upper epoch limit (default: 10)\n","args.ksize = 7 # kernel size (default: 7)\n","args.levels = 8 # number of levels (default: 8)\n","args.seq_len = 400 # sequence length (default: 400)\n","args.log_interval = 100 # report interval (default: 100)\n","args.lr = 4e-3 # initial learning rate (default: 4e-3)\n","args.optim = 'Adam' # optimizer to use (default: Adam)\n","args.nhid = 30 # # of hidden units per layer (default: 30)\n","args.seed = 1111 # random seed (default: 1111)\n","\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","input_channels = 2\n","n_classes = 1\n","batch_size = args.batch_size\n","seq_length = args.seq_len\n","epochs = args.epochs\n","\n","print(args)\n","print(\"Producing data...\")\n","X_train, Y_train = data_generator(50000, seq_length)\n","X_test, Y_test = data_generator(1000, seq_length)\n","\n","# Note: We use a very simple setting here (assuming all levels have the same # of channels.\n","channel_sizes = [args.nhid]*args.levels\n","kernel_size = args.ksize\n","dropout = args.dropout\n","model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=dropout)\n","\n","if args.cuda:\n","    model.cuda()\n","    X_train = X_train.cuda()\n","    Y_train = Y_train.cuda()\n","    X_test = X_test.cuda()\n","    Y_test = Y_test.cuda()\n","\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def train(epoch):\n","    global lr\n","    model.train()\n","    batch_idx = 1\n","    total_loss = 0\n","    for i in range(0, X_train.size(0), batch_size):\n","        if i + batch_size > X_train.size(0):\n","            x, y = X_train[i:], Y_train[i:]\n","        else:\n","            x, y = X_train[i:(i+batch_size)], Y_train[i:(i+batch_size)]\n","        optimizer.zero_grad()\n","        output = model(x)\n","        loss = F.mse_loss(output, y)\n","        loss.backward()\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        optimizer.step()\n","        batch_idx += 1\n","        total_loss += loss.item()\n","\n","        if batch_idx % args.log_interval == 0:\n","            cur_loss = total_loss / args.log_interval\n","            processed = min(i+batch_size, X_train.size(0))\n","            print('Train Epoch: {:2d} [{:6d}/{:6d} ({:.0f}%)]\\tLearning rate: {:.4f}\\tLoss: {:.6f}'.format(\n","                epoch, processed, X_train.size(0), 100.*processed/X_train.size(0), lr, cur_loss))\n","            total_loss = 0\n","\n","\n","def evaluate():\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(X_test)\n","        test_loss = F.mse_loss(output, Y_test)\n","        print('\\nTest set: Average loss: {:.6f}\\n'.format(test_loss.item()))\n","        return test_loss.item()\n","\n","\n","for ep in range(1, epochs+1):\n","    train(ep)\n","    tloss = evaluate()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7fe0715b2fd0>\n","Producing data...\n","Train Epoch:  1 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.323252\n","Train Epoch:  1 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.172877\n","Train Epoch:  1 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.172430\n","Train Epoch:  1 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.179994\n","Train Epoch:  1 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.167609\n","Train Epoch:  1 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.173014\n","Train Epoch:  1 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.167661\n","Train Epoch:  1 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.165028\n","Train Epoch:  1 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.168335\n","Train Epoch:  1 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.169019\n","Train Epoch:  1 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.166602\n","Train Epoch:  1 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.174442\n","Train Epoch:  1 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.156246\n","Train Epoch:  1 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.167535\n","Train Epoch:  1 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.163999\n","\n","Test set: Average loss: 0.172587\n","\n","Train Epoch:  2 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.159734\n","Train Epoch:  2 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.161702\n","Train Epoch:  2 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.165307\n","Train Epoch:  2 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.173285\n","Train Epoch:  2 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.163247\n","Train Epoch:  2 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.175357\n","Train Epoch:  2 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.163448\n","Train Epoch:  2 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.158256\n","Train Epoch:  2 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.095515\n","Train Epoch:  2 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.006763\n","Train Epoch:  2 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.003897\n","Train Epoch:  2 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.003745\n","Train Epoch:  2 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.002633\n","Train Epoch:  2 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.002336\n","Train Epoch:  2 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.002193\n","\n","Test set: Average loss: 0.001887\n","\n","Train Epoch:  3 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.002334\n","Train Epoch:  3 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.001788\n","Train Epoch:  3 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.001641\n","Train Epoch:  3 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.001976\n","Train Epoch:  3 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.001423\n","Train Epoch:  3 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.001384\n","Train Epoch:  3 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.002032\n","Train Epoch:  3 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.001246\n","Train Epoch:  3 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000897\n","Train Epoch:  3 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000750\n","Train Epoch:  3 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.001497\n","Train Epoch:  3 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.001300\n","Train Epoch:  3 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.001066\n","Train Epoch:  3 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.001096\n","Train Epoch:  3 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.001276\n","\n","Test set: Average loss: 0.000796\n","\n","Train Epoch:  4 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.001003\n","Train Epoch:  4 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000955\n","Train Epoch:  4 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000783\n","Train Epoch:  4 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.001354\n","Train Epoch:  4 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.001035\n","Train Epoch:  4 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.001065\n","Train Epoch:  4 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.001583\n","Train Epoch:  4 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.001087\n","Train Epoch:  4 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000681\n","Train Epoch:  4 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000481\n","Train Epoch:  4 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.001051\n","Train Epoch:  4 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.001119\n","Train Epoch:  4 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000932\n","Train Epoch:  4 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000794\n","Train Epoch:  4 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.001192\n","\n","Test set: Average loss: 0.000544\n","\n","Train Epoch:  5 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000743\n","Train Epoch:  5 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000932\n","Train Epoch:  5 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000551\n","Train Epoch:  5 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000819\n","Train Epoch:  5 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000853\n","Train Epoch:  5 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000579\n","Train Epoch:  5 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.001218\n","Train Epoch:  5 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.001150\n","Train Epoch:  5 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000375\n","Train Epoch:  5 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000267\n","Train Epoch:  5 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000704\n","Train Epoch:  5 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000694\n","Train Epoch:  5 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000494\n","Train Epoch:  5 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000540\n","Train Epoch:  5 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000761\n","\n","Test set: Average loss: 0.000377\n","\n","Train Epoch:  6 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000301\n","Train Epoch:  6 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000366\n","Train Epoch:  6 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000364\n","Train Epoch:  6 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000408\n","Train Epoch:  6 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000545\n","Train Epoch:  6 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.001001\n","Train Epoch:  6 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.001344\n","Train Epoch:  6 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000850\n","Train Epoch:  6 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000361\n","Train Epoch:  6 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000208\n","Train Epoch:  6 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000310\n","Train Epoch:  6 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000348\n","Train Epoch:  6 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000294\n","Train Epoch:  6 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000235\n","Train Epoch:  6 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000362\n","\n","Test set: Average loss: 0.000082\n","\n","Train Epoch:  7 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000201\n","Train Epoch:  7 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000296\n","Train Epoch:  7 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000206\n","Train Epoch:  7 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000241\n","Train Epoch:  7 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000319\n","Train Epoch:  7 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000463\n","Train Epoch:  7 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000273\n","Train Epoch:  7 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000293\n","Train Epoch:  7 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000346\n","Train Epoch:  7 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000255\n","Train Epoch:  7 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000272\n","Train Epoch:  7 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000234\n","Train Epoch:  7 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000209\n","Train Epoch:  7 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000137\n","Train Epoch:  7 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000579\n","\n","Test set: Average loss: 0.000417\n","\n","Train Epoch:  8 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000237\n","Train Epoch:  8 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000239\n","Train Epoch:  8 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000181\n","Train Epoch:  8 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000134\n","Train Epoch:  8 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000223\n","Train Epoch:  8 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000450\n","Train Epoch:  8 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000965\n","Train Epoch:  8 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000269\n","Train Epoch:  8 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000220\n","Train Epoch:  8 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000092\n","Train Epoch:  8 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000206\n","Train Epoch:  8 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000351\n","Train Epoch:  8 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000113\n","Train Epoch:  8 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000089\n","Train Epoch:  8 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000599\n","\n","Test set: Average loss: 0.000237\n","\n","Train Epoch:  9 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000355\n","Train Epoch:  9 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000151\n","Train Epoch:  9 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000109\n","Train Epoch:  9 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000122\n","Train Epoch:  9 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000125\n","Train Epoch:  9 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000205\n","Train Epoch:  9 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000510\n","Train Epoch:  9 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000225\n","Train Epoch:  9 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000107\n","Train Epoch:  9 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000129\n","Train Epoch:  9 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000086\n","Train Epoch:  9 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000966\n","Train Epoch:  9 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000441\n","Train Epoch:  9 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000063\n","Train Epoch:  9 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000114\n","\n","Test set: Average loss: 0.000387\n","\n","Train Epoch: 10 [  3168/ 50000 (6%)]\tLearning rate: 0.0040\tLoss: 0.000288\n","Train Epoch: 10 [  6368/ 50000 (13%)]\tLearning rate: 0.0040\tLoss: 0.000218\n","Train Epoch: 10 [  9568/ 50000 (19%)]\tLearning rate: 0.0040\tLoss: 0.000110\n","Train Epoch: 10 [ 12768/ 50000 (26%)]\tLearning rate: 0.0040\tLoss: 0.000049\n","Train Epoch: 10 [ 15968/ 50000 (32%)]\tLearning rate: 0.0040\tLoss: 0.000088\n","Train Epoch: 10 [ 19168/ 50000 (38%)]\tLearning rate: 0.0040\tLoss: 0.000105\n","Train Epoch: 10 [ 22368/ 50000 (45%)]\tLearning rate: 0.0040\tLoss: 0.000709\n","Train Epoch: 10 [ 25568/ 50000 (51%)]\tLearning rate: 0.0040\tLoss: 0.000293\n","Train Epoch: 10 [ 28768/ 50000 (58%)]\tLearning rate: 0.0040\tLoss: 0.000137\n","Train Epoch: 10 [ 31968/ 50000 (64%)]\tLearning rate: 0.0040\tLoss: 0.000052\n","Train Epoch: 10 [ 35168/ 50000 (70%)]\tLearning rate: 0.0040\tLoss: 0.000086\n","Train Epoch: 10 [ 38368/ 50000 (77%)]\tLearning rate: 0.0040\tLoss: 0.000567\n","Train Epoch: 10 [ 41568/ 50000 (83%)]\tLearning rate: 0.0040\tLoss: 0.000543\n","Train Epoch: 10 [ 44768/ 50000 (90%)]\tLearning rate: 0.0040\tLoss: 0.000069\n","Train Epoch: 10 [ 47968/ 50000 (96%)]\tLearning rate: 0.0040\tLoss: 0.000108\n","\n","Test set: Average loss: 0.000055\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jqTWGWYWA6gt","colab_type":"code","outputId":"04eb2ae0-9e0f-4fd4-ceac-878d25d605e1","executionInfo":{"status":"ok","timestamp":1557155056863,"user_tz":-120,"elapsed":8994,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["import random\n","\n","model.eval()\n","with torch.no_grad():\n","    output = model(X_test)\n","\n","random_index = random.randint(0, X_test.cpu().shape[0])\n","summands = [X_test[random_index,0,i] for i in range(X_test.cpu().shape[2])\n","            if X_test[random_index,1,i].item() == 1]\n","for s in summands:\n","    print(\" + {:.4f}\".format(s))\n","print(\"__________\")\n","print(\" = {:.4f} (prediction)\".format(Y_test[random_index].item()))\n","print(\" = {:.4f} (actual)\".format(output[random_index].item()))"],"execution_count":26,"outputs":[{"output_type":"stream","text":[" + 0.4731\n"," + 0.6803\n","__________\n"," = 1.1534 (prediction)\n"," = 1.1566 (actual)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5Fee1dwMZkXk","colab_type":"text"},"source":["# Char CNN"]},{"cell_type":"markdown","metadata":{"id":"uIvisR9DJ-wV","colab_type":"text"},"source":["In character-level language modeling tasks, each sequence is broken into elements by characters. \n","Therefore, in a character-level language model, at each time step the model is expected to predict\n","the next coming character. We evaluate the temporal convolutional network as a character-level\n","language model on the PennTreebank dataset and the text8 dataset.\n"]},{"cell_type":"markdown","metadata":{"id":"UjIaPkp__4Ud","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"6WhNTwiN_4u0","colab_type":"code","outputId":"e57d9781-6437-431c-b102-14f1c92cc328","executionInfo":{"status":"ok","timestamp":1556811457354,"user_tz":-120,"elapsed":7129,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":112}},"source":["!pip install unidecode\n","!pip install observations\n","\n","\n","import unidecode\n","import torch\n","from torch.autograd import Variable\n","from collections import Counter\n","import observations\n","import os\n","import pickle\n","\n","\n","cuda = torch.cuda.is_available()\n","\n","\n","def data_generator(args):\n","    file, testfile, valfile = getattr(observations, args.dataset)(base_dir + 'data/')\n","    file_len = len(file)\n","    valfile_len = len(valfile)\n","    testfile_len = len(testfile)\n","    corpus = Corpus(file + \" \" + valfile + \" \" + testfile)\n","\n","    #############################################################\n","    # Use the following if you want to pickle the loaded data\n","    #\n","    pickle_name = \"{0}.corpus\".format(args.dataset)\n","    if os.path.exists(pickle_name):\n","        corpus = pickle.load(open(pickle_name, 'rb'))\n","    else:\n","        corpus = Corpus(file + \" \" + valfile + \" \" + testfile)\n","        pickle.dump(corpus, open(pickle_name, 'wb'))\n","    #############################################################\n","\n","    return file, file_len, valfile, valfile_len, testfile, testfile_len, corpus\n","\n","\n","def read_file(filename):\n","    file = unidecode.unidecode(open(filename).read())\n","    return file, len(file)\n","\n","\n","class Dictionary(object):\n","    def __init__(self):\n","        self.char2idx = {}\n","        self.idx2char = []\n","        self.counter = Counter()\n","\n","    def add_word(self, char):\n","        self.counter[char] += 1\n","\n","    def prep_dict(self):\n","        for char in self.counter:\n","            if char not in self.char2idx:\n","                self.idx2char.append(char)\n","                self.char2idx[char] = len(self.idx2char) - 1\n","\n","    def __len__(self):\n","        return len(self.idx2char)\n","\n","\n","class Corpus(object):\n","    def __init__(self, string):\n","        self.dict = Dictionary()\n","        for c in string:\n","            self.dict.add_word(c)\n","        self.dict.prep_dict()\n","\n","\n","def char_tensor(corpus, string):\n","    tensor = torch.zeros(len(string)).long()\n","    for i in range(len(string)):\n","        tensor[i] = corpus.dict.char2idx[string[i]]\n","    return Variable(tensor).cuda() if cuda else Variable(tensor)\n","\n","\n","def batchify(data, batch_size, args):\n","    \"\"\"The output should have size [L x batch_size], where L could be a long sequence length\"\"\"\n","    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n","    nbatch = data.size(0) // batch_size\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * batch_size)\n","    # Evenly divide the data across the batch_size batches.\n","    data = data.view(batch_size, -1)\n","    if args.cuda:\n","        data = data.cuda()\n","    return data\n","\n","\n","def get_batch(source, start_index, args):\n","    seq_len = min(args.seq_len, source.size(1) - 1 - start_index)\n","    end_index = start_index + seq_len\n","    inp = source[:, start_index:end_index].contiguous()\n","    target = source[:, start_index+1:end_index+1].contiguous()  # The successors of the inp.\n","    return inp, target\n","\n","\n","def save(model):\n","    save_filename = 'model.pt'\n","    torch.save(model, save_filename)\n","    print('Saved as %s' % save_filename)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: unidecode in /usr/local/lib/python3.6/dist-packages (1.0.23)\n","Requirement already satisfied: observations in /usr/local/lib/python3.6/dist-packages (0.1.4)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from observations) (1.16.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from observations) (1.12.0)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fzH4GZjmjmIt","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"fErWInznsclF","colab_type":"code","colab":{}},"source":["from torch import nn\n","\n","\n","class TCN(nn.Module):\n","    def __init__(self, input_size, output_size, num_channels, kernel_size=2, dropout=0.2, emb_dropout=0.2):\n","        super(TCN, self).__init__()\n","        self.encoder = nn.Embedding(output_size, input_size)\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n","        self.decoder = nn.Linear(input_size, output_size)\n","        self.decoder.weight = self.encoder.weight\n","        self.drop = nn.Dropout(emb_dropout)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        initrange = 0.1\n","        self.encoder.weight.data.uniform_(-initrange, initrange)\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.uniform_(-initrange, initrange)\n","\n","    def forward(self, x):\n","        # input has dimension (N, L_in), and emb has dimension (N, L_in, C_in)\n","        emb = self.drop(self.encoder(x))\n","        y = self.tcn(emb.transpose(1, 2))\n","        o = self.decoder(y.transpose(1, 2))\n","        return o.contiguous()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GIe2aUvB_5Hm","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"XPn2UhrX_5kj","colab_type":"code","outputId":"204b9ad6-2719-4868-a51d-0be54175b9a6","executionInfo":{"status":"ok","timestamp":1556811520915,"user_tz":-120,"elapsed":68684,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":319}},"source":["import argparse\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import math\n","\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")   # Suppress the RunTimeWarning on unicode\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.batch_size = 32 # batch size (default: 32)\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.1 # dropout applied to layers (default: 0.1)\n","args.emb_dropout = 0.1 # dropout applied to the embedded layer (0 = no dropout) (default: 0.1)\n","args.clip = 0.15 # gradient clip, -1 means no clip (default: 0.15)\n","args.epochs = 100 # upper epoch limit (default: 100)\n","args.ksize = 3 # kernel size (default: 3)\n","args.levels = 3 # # of levels (default: 3)\n","args.log_interval = 100 # report interval (default: 100)\n","args.lr = 4 # initial learning rate (default: 4)\n","args.emsize = 100 # dimension of character embeddings (default: 100)\n","args.optim = 'SGD' # optimizer to use (default: SGD)\n","args.nhid = 450 # number of hidden units per layer (default: 450)\n","args.validseqlen = 320 # valid sequence length (default: 320)\n","args.seq_len = 400 # total sequence length, including effective history (default: 400)\n","args.seed = 1111 # random seed (default: 1111)\n","args.dataset = 'ptb' # dataset to use (default: ptb)\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","\n","print(args)\n","file, file_len, valfile, valfile_len, testfile, testfile_len, corpus = data_generator(args)\n","\n","n_characters = len(corpus.dict)\n","train_data = batchify(char_tensor(corpus, file), args.batch_size, args)\n","val_data = batchify(char_tensor(corpus, valfile), 1, args)\n","test_data = batchify(char_tensor(corpus, testfile), 1, args)\n","print(\"Corpus size: \", n_characters)\n","\n","\n","num_chans = [args.nhid] * (args.levels - 1) + [args.emsize]\n","k_size = args.ksize\n","dropout = args.dropout\n","emb_dropout = args.emb_dropout\n","model = TCN(args.emsize, n_characters, num_chans, kernel_size=k_size, dropout=dropout, emb_dropout=emb_dropout)\n","\n","\n","if args.cuda:\n","    model.cuda()\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def evaluate(source):\n","    model.eval()\n","    total_loss = 0\n","    count = 0\n","    source_len = source.size(1)\n","    with torch.no_grad():\n","        for batch, i in enumerate(range(0, source_len - 1, args.validseqlen)):\n","            if i + args.seq_len - args.validseqlen >= source_len:\n","                continue\n","            inp, target = get_batch(source, i, args)\n","            output = model(inp)\n","            eff_history = args.seq_len - args.validseqlen\n","            final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n","            final_target = target[:, eff_history:].contiguous().view(-1)\n","            loss = criterion(final_output, final_target)\n","\n","            total_loss += loss.data * final_output.size(0)\n","            count += final_output.size(0)\n","\n","        val_loss = total_loss.item() / count * 1.0\n","        return val_loss\n","\n","\n","def train(epoch):\n","    model.train()\n","    total_loss = 0\n","    start_time = time.time()\n","    losses = []\n","    source = train_data\n","    source_len = source.size(1)\n","    for batch_idx, i in enumerate(range(0, source_len - 1, args.validseqlen)):\n","        if i + args.seq_len - args.validseqlen >= source_len:\n","            continue\n","        inp, target = get_batch(source, i, args)\n","        optimizer.zero_grad()\n","        output = model(inp)\n","        eff_history = args.seq_len - args.validseqlen\n","        final_output = output[:, eff_history:].contiguous().view(-1, n_characters)\n","        final_target = target[:, eff_history:].contiguous().view(-1)\n","        loss = criterion(final_output, final_target)\n","        loss.backward()\n","\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n","            cur_loss = total_loss / args.log_interval\n","            losses.append(cur_loss)\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.2f} | '\n","                  'loss {:5.3f} | bpc {:5.3f}'.format(\n","                epoch, batch_idx, int((source_len-0.5) / args.validseqlen), lr,\n","                              elapsed * 1000 / args.log_interval, cur_loss, cur_loss / math.log(2)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","    return sum(losses) * 1.0 / len(losses)\n","\n","\n","def main():\n","    global lr\n","    try:\n","        print(\"Training for %d epochs...\" % args.epochs)\n","        all_losses = []\n","        best_vloss = 1e7\n","        for epoch in range(1, args.epochs + 1):\n","            loss = train(epoch)\n","\n","            vloss = evaluate(val_data)\n","            print('-' * 89)\n","            print('| End of epoch {:3d} | valid loss {:5.3f} | valid bpc {:8.3f}'.format(\n","                epoch, vloss, vloss / math.log(2)))\n","\n","            test_loss = evaluate(test_data)\n","            print('=' * 89)\n","            print('| End of epoch {:3d} | test loss {:5.3f} | test bpc {:8.3f}'.format(\n","                epoch, test_loss, test_loss / math.log(2)))\n","            print('=' * 89)\n","\n","            if epoch > 5 and vloss > max(all_losses[-3:]):\n","                lr = lr / 10.\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = lr\n","            all_losses.append(vloss)\n","\n","            if vloss < best_vloss:\n","                print(\"Saving...\")\n","                save(model)\n","                best_vloss = vloss\n","\n","    except KeyboardInterrupt:\n","        print('-' * 89)\n","        print(\"Saving before quit...\")\n","        save(model)\n","\n","    # Run on test data.\n","    test_loss = evaluate(test_data)\n","    print('=' * 89)\n","    print('| End of training | test loss {:5.3f} | test bpc {:8.3f}'.format(\n","        test_loss, test_loss / math.log(2)))\n","    print('=' * 89)\n","\n","# train_by_random_chunk()\n","if __name__ == \"__main__\":\n","    main()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7f005a7606a0>\n",">> Downloading /content/drive/My Drive/Colab Notebooks/TCN/data/simple-examples.tgz.part \n",">> [33.3 MB/33.3 MB] 102% @21.5 MB/s,[0s remaining, 1s elapsed]        \n","URL http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz downloaded to /content/drive/My Drive/Colab Notebooks/TCN/data/simple-examples.tgz \n","Corpus size:  49\n","Training for 100 epochs...\n","| epoch   1 |   100/  514 batches | lr 4.00000 | ms/batch 66.67 | loss 3.046 | bpc 4.395\n","| epoch   1 |   200/  514 batches | lr 4.00000 | ms/batch 65.79 | loss 2.530 | bpc 3.650\n","| epoch   1 |   300/  514 batches | lr 4.00000 | ms/batch 66.32 | loss 2.208 | bpc 3.186\n","-----------------------------------------------------------------------------------------\n","Saving before quit...\n","Saved as model.pt\n","=========================================================================================\n","| End of training | test loss 2.037 | test bpc    2.939\n","=========================================================================================\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"STH78H5WZleL","colab_type":"text"},"source":["# Copy Memory"]},{"cell_type":"markdown","metadata":{"id":"1d2Idn_LJ7jo","colab_type":"text"},"source":["In this task, each input sequence has length T+20. The first 10 values are chosen randomly \n","among the digits 1-8, with the rest being all zeros, except for the last 11 entries that are \n","filled with the digit ‘9’ (the first ‘9’ is a delimiter). The goal is to generate an output \n","of same length that is zero everywhere, except the last 10 values after the delimiter, where \n","the model is expected to repeat the 10 values it encountered at the start of the input.\n"]},{"cell_type":"markdown","metadata":{"id":"Yv_NQyZFHWhg","colab_type":"text"},"source":["![alt text](https://previews.dropbox.com/p/thumb/AAYLhPxapMyJAr8NY_uPDNYkbBeZoJAKFpNnDGLfmRtAX7QTbn1gH1-Jya7BU7nO-30IbXOez7bYBM_7u_Mum-TkxfNCfYiDUC73YVgj27EXm-0KqwaCTm0Bs0TxbC6OjZmG_6-wNCq-ugT-mNZgAhm1LaehJfqTp7hn9Pd9yfWgSHqw8QIFmxAKsWHpGyYPiJ_SrzJ8LM1DJcn121A7WUwxlnzYh_p5_034z5Onw3TZDcxEIFYMKJFTnDCkYmycZOLE6I5jawtkOmpKkpOLR9s4ifqIfUiWp1jSoNVCGHd6OWnKgyXP08QghqU0U2Owf5mizW_RuH63959LZtWHFgw1/p.png?size_mode=5)"]},{"cell_type":"markdown","metadata":{"id":"rHqaqJGkHqpS","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"9eJo4pScHrAE","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","from torch.autograd import Variable\n","\n","\n","def data_generator(T, mem_length, b_size):\n","    \"\"\"\n","    Generate data for the copying memory task\n","\n","    :param T: The total blank time length\n","    :param mem_length: The length of the memory to be recalled\n","    :param b_size: The batch size\n","    :return: Input and target data tensor\n","    \"\"\"\n","    seq = torch.from_numpy(np.random.randint(1, 9, size=(b_size, mem_length))).float()\n","    zeros = torch.zeros((b_size, T))\n","    marker = 9 * torch.ones((b_size, mem_length + 1))\n","    placeholders = torch.zeros((b_size, mem_length))\n","\n","    x = torch.cat((seq, zeros[:, :-1], marker), 1)\n","    y = torch.cat((placeholders, zeros, seq), 1).long()\n","\n","    x, y = Variable(x), Variable(y)\n","    return x, y"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3orDKktHrlu","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"t48RIWpsHr8R","colab_type":"code","colab":{}},"source":["from torch import nn\n","\n","\n","class TCN(nn.Module):\n","    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n","        super(TCN, self).__init__()\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n","        self.linear = nn.Linear(num_channels[-1], output_size)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.linear.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, x):\n","        y1 = self.tcn(x)\n","        return self.linear(y1.transpose(1, 2))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLchbP_Fj2R7","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"BqEFjmpnsctE","colab_type":"code","outputId":"d7371dec-fce0-4995-ad1f-621eae6c2479","executionInfo":{"status":"error","timestamp":1556811539768,"user_tz":-120,"elapsed":18355,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":971}},"source":["import argparse\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","import numpy as np\n","import time\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.batch_size = 32 # batch size (default: 32)\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.0 # dropout applied to layers (default: 0.0)\n","args.clip = 1.0 # gradient clip, -1 means no clip (default: 1.0)\n","args.epochs = 50 # upper epoch limit (default: 50)\n","args.ksize = 8 # kernel size (default: 8)\n","args.iters = 100 # number of iters per epoch (default: 100)\n","args.levels = 8 # # of levels (default: 8)\n","args.blank_len = 1000 # The size of the blank (i.e. T) (default: 1000)\n","args.seq_len = 10 # initial history size (default: 10)\n","args.log_interval = 50 # report interval (default: 50)\n","args.lr = 5e-4 # initial learning rate (default: 5e-4)\n","args.optim = 'RMSprop' # optimizer to use (default: RMSprop)\n","args.nhid = 10 # number of hidden units per layer (default: 10)\n","args.seed = 1111 # random seed (default: 1111)\n","\n","\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","\n","batch_size = args.batch_size\n","seq_len = args.seq_len    # The size to memorize\n","epochs = args.epochs\n","iters = args.iters\n","T = args.blank_len\n","n_steps = T + (2 * seq_len)\n","n_classes = 10  # Digits 0 - 9\n","n_train = 10000\n","n_test = 1000\n","\n","print(args)\n","print(\"Preparing data...\")\n","train_x, train_y = data_generator(T, seq_len, n_train)\n","test_x, test_y = data_generator(T, seq_len, n_test)\n","\n","\n","channel_sizes = [args.nhid] * args.levels\n","kernel_size = args.ksize\n","dropout = args.dropout\n","model = TCN(1, n_classes, channel_sizes, kernel_size, dropout=dropout)\n","\n","if args.cuda:\n","    model.cuda()\n","    train_x = train_x.cuda()\n","    train_y = train_y.cuda()\n","    test_x = test_x.cuda()\n","    test_y = test_y.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def evaluate():\n","    model.eval()\n","    with torch.no_grad():\n","        out = model(test_x.unsqueeze(1).contiguous())\n","        loss = criterion(out.view(-1, n_classes), test_y.view(-1))\n","        pred = out.view(-1, n_classes).data.max(1, keepdim=True)[1]\n","        correct = pred.eq(test_y.data.view_as(pred)).cpu().sum()\n","        counter = out.view(-1, n_classes).size(0)\n","        print('\\nTest set: Average loss: {:.8f}  |  Accuracy: {:.4f}\\n'.format(\n","            loss.item(), 100. * correct / counter))\n","        return loss.item()\n","\n","\n","def train(ep):\n","    global batch_size, seq_len, iters, epochs\n","    model.train()\n","    total_loss = 0\n","    start_time = time.time()\n","    correct = 0\n","    counter = 0\n","    for batch_idx, batch in enumerate(range(0, n_train, batch_size)):\n","        start_ind = batch\n","        end_ind = start_ind + batch_size\n","\n","        x = train_x[start_ind:end_ind]\n","        y = train_y[start_ind:end_ind]\n","        \n","        optimizer.zero_grad()\n","        out = model(x.unsqueeze(1).contiguous())\n","        loss = criterion(out.view(-1, n_classes), y.view(-1))\n","        pred = out.view(-1, n_classes).data.max(1, keepdim=True)[1]\n","        correct += pred.eq(y.data.view_as(pred)).cpu().sum()\n","        counter += out.view(-1, n_classes).size(0)\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        if batch_idx > 0 and batch_idx % args.log_interval == 0:\n","            avg_loss = total_loss / args.log_interval\n","            elapsed = time.time() - start_time\n","            print('| Epoch {:3d} | {:5d}/{:5d} batches | lr {:2.5f} | ms/batch {:5.2f} | '\n","                  'loss {:5.8f} | accuracy {:5.4f}'.format(\n","                ep, batch_idx, n_train // batch_size+1, args.lr, elapsed * 1000 / args.log_interval,\n","                avg_loss, 100. * correct / counter))\n","            start_time = time.time()\n","            total_loss = 0\n","            correct = 0\n","            counter = 0\n","\n","\n","for ep in range(1, epochs + 1):\n","    train(ep)\n","    evaluate()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7f005a68fa20>\n","Preparing data...\n","| Epoch   1 |    50/  313 batches | lr 0.00050 | ms/batch 16.51 | loss 0.25274376 | accuracy 94.0000\n","| Epoch   1 |   100/  313 batches | lr 0.00050 | ms/batch 16.10 | loss 0.05605565 | accuracy 98.0000\n","| Epoch   1 |   150/  313 batches | lr 0.00050 | ms/batch 17.31 | loss 0.03771827 | accuracy 99.0000\n","| Epoch   1 |   200/  313 batches | lr 0.00050 | ms/batch 17.48 | loss 0.02787946 | accuracy 99.0000\n","| Epoch   1 |   250/  313 batches | lr 0.00050 | ms/batch 17.63 | loss 0.02401529 | accuracy 99.0000\n","| Epoch   1 |   300/  313 batches | lr 0.00050 | ms/batch 17.29 | loss 0.02267975 | accuracy 99.0000\n","\n","Test set: Average loss: 0.02179394  |  Accuracy: 99.0000\n","\n","| Epoch   2 |    50/  313 batches | lr 0.00050 | ms/batch 17.92 | loss 0.02233413 | accuracy 99.0000\n","| Epoch   2 |   100/  313 batches | lr 0.00050 | ms/batch 17.46 | loss 0.02156935 | accuracy 99.0000\n","| Epoch   2 |   150/  313 batches | lr 0.00050 | ms/batch 17.45 | loss 0.02129342 | accuracy 99.0000\n","| Epoch   2 |   200/  313 batches | lr 0.00050 | ms/batch 17.55 | loss 0.02094540 | accuracy 99.0000\n","| Epoch   2 |   250/  313 batches | lr 0.00050 | ms/batch 17.54 | loss 0.02083528 | accuracy 99.0000\n","| Epoch   2 |   300/  313 batches | lr 0.00050 | ms/batch 17.52 | loss 0.02044723 | accuracy 99.0000\n","\n","Test set: Average loss: 0.02023196  |  Accuracy: 99.0000\n","\n","| Epoch   3 |    50/  313 batches | lr 0.00050 | ms/batch 17.72 | loss 0.02041873 | accuracy 99.0000\n","| Epoch   3 |   100/  313 batches | lr 0.00050 | ms/batch 16.06 | loss 0.01969369 | accuracy 99.0000\n","| Epoch   3 |   150/  313 batches | lr 0.00050 | ms/batch 16.31 | loss 0.01895363 | accuracy 99.0000\n","| Epoch   3 |   200/  313 batches | lr 0.00050 | ms/batch 15.99 | loss 0.01803284 | accuracy 99.0000\n","| Epoch   3 |   250/  313 batches | lr 0.00050 | ms/batch 16.08 | loss 0.01622877 | accuracy 99.0000\n","| Epoch   3 |   300/  313 batches | lr 0.00050 | ms/batch 16.35 | loss 0.01423393 | accuracy 99.0000\n","\n","Test set: Average loss: 0.01347971  |  Accuracy: 99.0000\n","\n","| Epoch   4 |    50/  313 batches | lr 0.00050 | ms/batch 16.78 | loss 0.01262144 | accuracy 99.0000\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-c9bb9ee2116c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-30-c9bb9ee2116c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ep)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-ad194673601b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtcn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-12e6ebecc15a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-0393e3b4ebb2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         return F.conv1d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 187\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"4foekOdRZmBk","colab_type":"text"},"source":["# Lambada Language"]},{"cell_type":"markdown","metadata":{"id":"uwfHpACbj4Uw","colab_type":"text"},"source":["LAMBADA is a collection of narrative passages sharing the characteristics such that human subjects are able to guess accurately given sufficient context, but not so if they only see the last sentence containing the target word. On average, the context contains 4.6 sentences, and the testing performance is evaluated by having the model the last element of the target sentence (i.e. the very last word). \n","\n","Most of the existing computational models fail on this task (without the help of external memory unit, such as neural cache). See [the original LAMBADA paper](https://arxiv.org/pdf/1606.06031.pdf) for more results on applying RNNs on LAMBADA.\n","\n","**Example**: \n","```\n","Context: “Yes, I thought I was going to lose the baby.” “I was scared too,” he stated, sincerity flooding his eyes. “You were ?” “Yes, of course. Why do you even ask?” “This baby wasn’t exactly planned for.”\n","\n","Target sentence: “Do you honestly think that I would want you to have a _______” \n","\n","Target word: miscarriage\n","```"]},{"cell_type":"markdown","metadata":{"id":"u_Zo2TWhNuZc","colab_type":"text"},"source":["NOT WORKING! DATA NOT DOWNLOADABLE AND NOT AVAILABLE!"]},{"cell_type":"markdown","metadata":{"id":"qbCKxmgLLXEE","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"yzwuWwdtsc1f","colab_type":"code","colab":{}},"source":["import os\n","import torch\n","from torch.autograd import Variable\n","import re\n","from collections import Counter\n","import pickle\n","\n","\"\"\"\n","Note: The meaning of batch_size in PTB is different from that in MNIST example. In MNIST, \n","batch_size is the # of sample data that is considered in each iteration; in PTB, however,\n","it is the number of segments to speed up computation. \n","\n","The goal of PTB is to train a language model to predict the next word.\n","\"\"\"\n","\n","def data_generator(args):\n","    args.data = base_dir + args.data\n","    print(args.data)\n","    if os.path.exists(args.data + \"/corpus\") and not args.corpus:\n","        corpus = pickle.load(open(args.data + '/corpus', 'rb'))\n","    else:\n","        print(\"Creating Corpus...\")\n","        corpus = Corpus(args.data + \"/lambada_vocabulary_sorted.txt\", args.data)\n","        pickle.dump(corpus, open(args.data + '/corpus', 'wb'))\n","\n","    eval_batch_size = 1\n","    train_data = batchify(corpus.train, args.batch_size, args)\n","    val_data = [[0] * (args.seq_len-len(line)) + line for line in corpus.valid]\n","    test_data = [[0] * (args.seq_len-len(line)) + line for line in corpus.test]\n","    return train_data, val_data, test_data, corpus\n","\n","\n","class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)\n","\n","\n","class Corpus(object):\n","    def __init__(self, dict_path, path):\n","        self.dictionary = Dictionary()\n","        self.prep_dict(dict_path)\n","        self.train = torch.LongTensor(self.tokenize(os.path.join(path, 'train-novels')))\n","        self.valid = self.tokenize(os.path.join(path, 'lambada_development_plain_text.txt'), eval=True)\n","        self.test = self.tokenize(os.path.join(path, 'lambada_test_plain_text.txt'), eval=True)\n","\n","    def prep_dict(self, dict_path):\n","        assert os.path.exists(dict_path)\n","\n","        # Add words to the dictionary\n","        with open(dict_path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                word = line.strip()\n","                tokens += 1\n","                self.dictionary.add_word(word)\n","\n","        if \"<eos>\" not in self.dictionary.word2idx:\n","            self.dictionary.add_word(\"<eos>\")\n","            tokens += 1\n","\n","        print(\"The dictionary captured a vocabulary of size {0}.\".format(tokens))\n","\n","    def tokenize(self, path, eval=False):\n","        assert os.path.exists(path)\n","\n","        ids = []\n","        token = 0\n","        misses = 0\n","        if not path.endswith(\".txt\"):   # it's a folder\n","            for subdir in os.listdir(path):\n","                for filename in os.listdir(path + \"/\" + subdir):\n","                    if filename.endswith(\".txt\"):\n","                        full_path = \"{0}/{1}/{2}\".format(path, subdir, filename)\n","                        # Tokenize file content\n","                        delta_ids, delta_token, delta_miss = self._tokenize_file(full_path, eval=eval)\n","                        ids += delta_ids\n","                        token += delta_token\n","                        misses += delta_miss\n","        else:\n","            ids, token, misses = self._tokenize_file(path, eval=eval)\n","\n","        print(token, misses)\n","        return ids\n","\n","    def _tokenize_file(self, path, eval=False):\n","        with open(path, 'r') as f:\n","            token = 0\n","            ids = []\n","            misses = 0\n","            for line in f:\n","                line_ids = []\n","                words = line.strip().split() + ['<eos>']\n","                if eval:\n","                    words = words[:-1]\n","                for word in words:\n","                    # These words are in the text but not vocabulary\n","                    if word == \"n't\":\n","                        word = \"not\"\n","                    elif word == \"'s\":\n","                        word = \"is\"\n","                    elif word == \"'re\":\n","                        word = \"are\"\n","                    elif word == \"'ve\":\n","                        word = \"have\"\n","                    elif word == \"wo\":\n","                        word = \"will\"\n","                    if word not in self.dictionary.word2idx:\n","                        word = re.sub(r'[^\\w\\s]', '', word)\n","                    if word not in self.dictionary.word2idx:\n","                        misses += 1\n","                        continue\n","                    line_ids.append(self.dictionary.word2idx[word])\n","                    token += 1\n","                if eval:\n","                    ids.append(line_ids)\n","                else:\n","                    ids += line_ids\n","        return ids, token, misses\n","\n","\n","def batchify(data, batch_size, args):\n","    \"\"\"The output should have size [L x batch_size], where L could be a long sequence length\"\"\"\n","    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n","    nbatch = data.size(0) // batch_size\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * batch_size)\n","    # Evenly divide the data across the batch_size batches.\n","    data = data.view(batch_size, -1)\n","    print(data.size())\n","    if args.cuda:\n","        data = data.cuda()\n","    return data\n","\n","\n","def get_batch(source, i, args, seq_len=None, evaluation=False):\n","    seq_len = min(seq_len if seq_len else args.seq_len, source.size(1) - 1 - i)\n","    data = Variable(source[:, i:i+seq_len], volatile=evaluation)\n","    target = Variable(source[:, i+1:i+1+seq_len])  # CAUTION: This is un-flattened!\n","    return data, target"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fi5mVVPtLX6L","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"1ukZadqQLYRS","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","\n","\n","class TCN(nn.Module):\n","\n","    def __init__(self, input_size, output_size, num_channels,\n","                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n","        super(TCN, self).__init__()\n","        self.encoder = nn.Embedding(output_size, input_size)\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n","\n","        self.decoder = nn.Linear(num_channels[-1], output_size)\n","        if tied_weights:\n","            if num_channels[-1] != input_size:\n","                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n","            self.decoder.weight = self.encoder.weight\n","            print(\"Weight tied\")\n","        self.drop = nn.Dropout(emb_dropout)\n","        self.emb_dropout = emb_dropout\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.encoder.weight.data.normal_(0, 0.01)\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, input):\n","        \"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\n","        emb = self.drop(self.encoder(input))\n","        y = self.tcn(emb.transpose(1, 2)).transpose(1, 2)\n","        y = self.decoder(y)\n","        return y.contiguous()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EN4V6i9kLYhe","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"revSDEQ4LY5V","colab_type":"code","outputId":"c73c7195-a748-4559-b4ce-41c3f3290012","executionInfo":{"status":"error","timestamp":1557148717329,"user_tz":-120,"elapsed":1518,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":453}},"source":["import argparse\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import pickle\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.batch_size = 20 # batch size (default: 20)\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.1 # dropout applied to layers (default: 0.1)\n","args.emb_dropout = 0.1 # dropout applied to the embedded layer (default: 0.1)\n","args.clip = 0.4 # gradient clip, -1 means no clip (default: 0.4)\n","args.epochs = 100 # upper epoch limit (default: 100)\n","args.ksize = 4 # kernel size (default: 4)\n","args.data = 'data/lambada' # location of the data corpus (default: ./data/lambada)\n","args.emsize = 500 # size of word embeddings (default: 500)\n","args.levels = 5 # # of levels (default: 5)\n","args.log_interval = 100 # report interval (default: 100)\n","args.lr = 4 # initial learning rate (default: 4)\n","args.nhid = 500 # number of hidden units per layer (default: 500)\n","args.seed = 1111 # random seed (default: 1111)\n","args.tied = True # tie the word embedding and softmax weights (default: True)\n","args.optim = 'SGD' # optimizer type (default: SGD)\n","args.validseqlen = 50 # valid sequence length (default: 50)\n","args.seq_len = 100 # total sequence length, including effective history (default: 100)\n","args.corpus = False # force re-make the corpus (default: False)\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","\n","print(args)\n","train_data, val_data, test_data, corpus = data_generator(args)\n","\n","n_words = len(corpus.dictionary)\n","print(\"Total # of words: {0}\".format(n_words))\n","\n","num_chans = [args.nhid] * (args.levels - 1) + [args.emsize]\n","k_size = args.ksize\n","dropout = args.dropout\n","emb_dropout = args.emb_dropout\n","tied = args.tied\n","\n","model = TCN(args.emsize, n_words, num_chans, dropout=dropout, \n","            emb_dropout=emb_dropout, kernel_size=k_size, tied_weights=tied)\n","\n","if args.cuda:\n","    model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0\n","    processed_data_size = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for i in range(len(data_source)):\n","            data, targets = torch.LongTensor(data_source[i]).view(1, -1), torch.LongTensor([data_source[i][-1]]).view(1, -1)\n","            data, targets = Variable(data), Variable(targets)\n","            if args.cuda:\n","                data, targets = data.cuda(), targets.cuda()\n","            output = model(data)\n","            final_output = output[:, -1].contiguous().view(-1, n_words)\n","            final_target = targets[:, -1].contiguous().view(-1)\n","            loss = criterion(final_output, final_target)\n","            total_loss += loss.data\n","            processed_data_size += 1\n","        return total_loss.item() / processed_data_size\n","\n","\n","def train():\n","    global train_data\n","    model.train()\n","    total_loss = 0\n","    start_time = time.time()\n","    for babatch_idxtch, i in enumerate(range(0, train_data.size(1) - 1, args.validseqlen)):\n","        if i + args.seq_len - args.validseqlen >= train_data.size(1) - 1:\n","            continue\n","        data, targets = get_batch(train_data, i, args)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        eff_history = args.seq_len - args.validseqlen\n","        if eff_history < 0:\n","            raise ValueError(\"Valid sequence length must be smaller than sequence length!\")\n","        final_target = targets[:, eff_history:].contiguous().view(-1)\n","        final_output = output[:, eff_history:].contiguous().view(-1, n_words)\n","        loss = criterion(final_output, final_target)\n","        loss.backward()\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n","            cur_loss = total_loss / args.log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.5f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch_idx, train_data.size(1) // args.validseqlen, lr,\n","                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            reg_loss = 0\n","            start_time = time.time()\n","\n","\n","if __name__ == \"__main__\":\n","    best_vloss = 1e8\n","    try:\n","        all_vloss = []\n","        for epoch in range(1, args.epochs+1):\n","            epoch_start_time = time.time()\n","            train()\n","            val_loss = evaluate(val_data)\n","            test_loss = evaluate(test_data)\n","            print('-' * 89)\n","            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                    'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                               val_loss, math.exp(val_loss)))\n","            print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n","                  'test ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                            test_loss, math.exp(test_loss)))\n","            print('-' * 89)\n","            # Save the model if the validation loss is the best we've seen so far.\n","\n","            if val_loss < best_vloss:\n","                with open(\"model.pt\", 'wb') as f:\n","                    print('Save model!\\n')\n","                    torch.save(model, f)\n","                best_vloss = val_loss\n","            if epoch > 5 and val_loss >= max(all_vloss[-5:]):\n","                lr = lr / 10.\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = lr\n","            all_vloss.append(val_loss)\n","\n","    except KeyboardInterrupt:\n","        print('-' * 89)\n","        print('Exiting from training early')\n","\n","    # Load the best saved model.\n","    with open(\"model.pt\", 'rb') as f:\n","        model = torch.load(f)\n","\n","    # Run on test data.\n","    test_loss = evaluate(test_data)\n","    print('=' * 89)\n","    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","        test_loss, math.exp(test_loss)))\n","    print('=' * 89)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7fe079140cc0>\n","/content/drive/My Drive/Colab Notebooks/TCN/data/lambada\n","Creating Corpus...\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-7dd257eed3e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-3cc9a619f97a>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating Corpus...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/lambada_vocabulary_sorted.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/corpus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-3cc9a619f97a>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dict_path, path)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprep_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train-novels'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lambada_development_plain_text.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-3cc9a619f97a>\u001b[0m in \u001b[0;36mprep_dict\u001b[0;34m(self, dict_path)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprep_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# Add words to the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"htG_1_XKZmon","colab_type":"text"},"source":["# MNIST Pixel"]},{"cell_type":"markdown","metadata":{"id":"2lkd1lCOj5Bt","colab_type":"text"},"source":["MNIST is a handwritten digit classification dataset (Lecun et al., 1998) that is frequently used to \n","test deep learning models. In particular, sequential MNIST is frequently used to test a recurrent \n","network’s ability to retain information from the distant past (see paper for references). In \n","this task, each MNIST image (28 x 28) is presented to the model as a 784 × 1 sequence \n","for digit classification. In the more challenging permuted MNIST (P-MNIST) setting, the order of \n","the sequence is permuted at a (fixed) random order."]},{"cell_type":"markdown","metadata":{"id":"KCgBivX4LZsD","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"7HJD9GWRsc9F","colab_type":"code","colab":{}},"source":["import torch\n","from torchvision import datasets, transforms\n","\n","\n","def data_generator(root, batch_size):\n","    train_set = datasets.MNIST(root=root, train=True, download=True,\n","                               transform=transforms.Compose([\n","                                   transforms.ToTensor(),\n","                                   transforms.Normalize((0.1307,), (0.3081,))\n","                               ]))\n","    test_set = datasets.MNIST(root=root, train=False, download=True,\n","                              transform=transforms.Compose([\n","                                  transforms.ToTensor(),\n","                                  transforms.Normalize((0.1307,), (0.3081,))\n","                              ]))\n","\n","    train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size)\n","    test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size)\n","    return train_loader, test_loader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ByuKgjkMLaG-","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"HFyyshM6Lae4","colab_type":"code","colab":{}},"source":["import torch.nn.functional as F\n","from torch import nn\n","\n","\n","class TCN(nn.Module):\n","    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n","        super(TCN, self).__init__()\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n","        self.linear = nn.Linear(num_channels[-1], output_size)\n","\n","    def forward(self, inputs):\n","        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n","        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n","        o = self.linear(y1[:, :, -1])\n","        return F.log_softmax(o, dim=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E09Bp1xwLa49","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"ai1gXqBVLeVT","colab_type":"code","outputId":"2d8fcbd2-883e-49ea-ef31-5d01868f8091","executionInfo":{"status":"error","timestamp":1556811983056,"user_tz":-120,"elapsed":101444,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":1121}},"source":["import torch\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","import argparse\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.batch_size = 64 # batch size (default: 64)\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.05 # dropout applied to layers (default: 0.05)\n","args.clip = -1 # gradient clip, -1 means no clip (default: -1)\n","args.epochs = 20 # upper epoch limit (default: 20)\n","args.ksize = 7 # kernel size (default: 7)\n","args.levels = 8 # # of levels (default: 8)\n","args.log_interval = 100 # report interval (default: 100)\n","args.lr = 2e-3 # initial learning rate (default: 2e-3)\n","args.optim = 'Adam' # optimizer to use (default: Adam)\n","args.nhid = 25 # number of hidden units per layer (default: 25)\n","args.seed = 1111 # random seed (default: 1111)\n","args.permute = False # use permuted MNIST (default: false)\n","\n","\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","\n","root = './data/mnist'\n","batch_size = args.batch_size\n","n_classes = 10\n","input_channels = 1\n","seq_length = int(784 / input_channels)\n","epochs = args.epochs\n","steps = 0\n","\n","print(args)\n","train_loader, test_loader = data_generator(root, batch_size)\n","\n","permute = torch.Tensor(np.random.permutation(784).astype(np.float64)).long()\n","channel_sizes = [args.nhid] * args.levels\n","kernel_size = args.ksize\n","model = TCN(input_channels, n_classes, channel_sizes, kernel_size=kernel_size, dropout=args.dropout)\n","\n","if args.cuda:\n","    model.cuda()\n","    permute = permute.cuda()\n","\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def train(ep):\n","    global steps\n","    train_loss = 0\n","    model.train()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        if args.cuda: data, target = data.cuda(), target.cuda()\n","        data = data.view(-1, input_channels, seq_length)\n","        if args.permute:\n","            data = data[:, :, permute]\n","        data, target = Variable(data), Variable(target)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        optimizer.step()\n","        train_loss += loss\n","        steps += seq_length\n","        if batch_idx > 0 and batch_idx % args.log_interval == 0:\n","            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tSteps: {}'.format(\n","                ep, batch_idx * batch_size, len(train_loader.dataset),\n","                100. * batch_idx / len(train_loader), train_loss.item()/args.log_interval, steps))\n","            train_loss = 0\n","\n","\n","def test():\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            if args.cuda:\n","                data, target = data.cuda(), target.cuda()\n","            data = data.view(-1, input_channels, seq_length)\n","            if args.permute:\n","                data = data[:, :, permute]\n","            data, target = Variable(data, volatile=True), Variable(target)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, size_average=False).item()\n","            pred = output.data.max(1, keepdim=True)[1]\n","            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n","\n","        test_loss /= len(test_loader.dataset)\n","        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, correct, len(test_loader.dataset),\n","            100. * correct / len(test_loader.dataset)))\n","        return test_loss\n","\n","\n","if __name__ == \"__main__\":\n","    for epoch in range(1, epochs+1):\n","        train(epoch)\n","        test()\n","        if epoch % 10 == 0:\n","            lr /= 10\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7f005a5fb2b0>\n","Train Epoch: 1 [6400/60000 (11%)]\tLoss: 1.142773\tSteps: 79184\n","Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.456071\tSteps: 157584\n","Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.321673\tSteps: 235984\n","Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.270216\tSteps: 314384\n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.238108\tSteps: 392784\n","Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.213016\tSteps: 471184\n","Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.201158\tSteps: 549584\n","Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.202662\tSteps: 627984\n","Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.182184\tSteps: 706384\n","\n","Test set: Average loss: 0.1764, Accuracy: 9433/10000 (94%)\n","\n","Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.143217\tSteps: 814576\n","Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.163855\tSteps: 892976\n","Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.144393\tSteps: 971376\n","Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.130545\tSteps: 1049776\n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.132044\tSteps: 1128176\n","Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.122866\tSteps: 1206576\n","Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.132582\tSteps: 1284976\n","Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.143445\tSteps: 1363376\n","Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.108644\tSteps: 1441776\n","\n","Test set: Average loss: 0.1139, Accuracy: 9620/10000 (96%)\n","\n","Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.102961\tSteps: 1549968\n","Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.117073\tSteps: 1628368\n","Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.102673\tSteps: 1706768\n","Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.104315\tSteps: 1785168\n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.102197\tSteps: 1863568\n","Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.101454\tSteps: 1941968\n","Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.108245\tSteps: 2020368\n","Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.117074\tSteps: 2098768\n","Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.088303\tSteps: 2177168\n","\n","Test set: Average loss: 0.0742, Accuracy: 9773/10000 (97%)\n","\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-42-a91cb80bf615>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-a91cb80bf615>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ep)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"4Y2TnFi8Ekz9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Dm-8X7xjuHr","colab_type":"text"},"source":["# Poly Music"]},{"cell_type":"markdown","metadata":{"id":"6aiodOHDj5m8","colab_type":"text"},"source":["We evaluate temporal convolutional network (TCN) on two popular polyphonic music dataset, described below.\n","\n","- **JSB Chorales** dataset (Allan & Williams, 2005) is a polyphonic music dataset con-\n","sisting of the entire corpus of 382 four-part harmonized chorales by J. S. Bach. In a polyphonic\n","music dataset, each input is a sequence of elements having 88 dimensions, representing the 88 keys\n","on a piano. Therefore, each element `x_t` is a chord written in as binary vector, in which a “1” indicates\n","a key pressed.\n","\n","- **Nottingham** dataset is a collection of 1200 British and American folk tunes. Not-\n","tingham is a much larger dataset than JSB Chorales. Along with JSB Chorales, Nottingham has\n","been used in a number of works that investigated recurrent models’ applicability in polyphonic mu-\n","sic, and the performance for both tasks are measured in terms\n","of negative log-likelihood (NLL) loss.\n","\n","The goal here is to predict the next note given some history of the notes played."]},{"cell_type":"markdown","metadata":{"id":"imhcpV9RLgb3","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"2h24GITLLgXm","colab_type":"code","colab":{}},"source":["from scipy.io import loadmat\n","import torch\n","import numpy as np\n","\n","\n","def data_generator(dataset):\n","    if dataset == \"JSB\":\n","        print('loading JSB data...')\n","        data = loadmat(base_dir + 'mdata/JSB_Chorales.mat')\n","    elif dataset == \"Muse\":\n","        print('loading Muse data...')\n","        data = loadmat(base_dir + 'mdata/MuseData.mat')\n","    elif dataset == \"Nott\":\n","        print('loading Nott data...')\n","        data = loadmat(base_dir + 'mdata/Nottingham.mat')\n","    elif dataset == \"Piano\":\n","        print('loading Piano data...')\n","        data = loadmat(base_dir + 'mdata/Piano_midi.mat')\n","\n","    X_train = data['traindata'][0]\n","    X_valid = data['validdata'][0]\n","    X_test = data['testdata'][0]\n","\n","    for data in [X_train, X_valid, X_test]:\n","        for i in range(len(data)):\n","            data[i] = torch.Tensor(data[i].astype(np.float64))\n","\n","    return X_train, X_valid, X_test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AbTen5wPLgTN","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"HJAb10ijLgO1","colab_type":"code","colab":{}},"source":["from torch import nn\n","import torch.nn.functional as F\n","\n","\n","class TCN(nn.Module):\n","    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n","        super(TCN, self).__init__()\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n","        self.linear = nn.Linear(num_channels[-1], output_size)\n","        self.sig = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # x needs to have dimension (N, C, L) in order to be passed into CNN\n","        output = self.tcn(x.transpose(1, 2)).transpose(1, 2)\n","        output = self.linear(output).double()\n","        return self.sig(output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juf3phNWLgKd","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"KbC9ep3Kjukj","colab_type":"code","outputId":"5bfa7b1e-39b3-46c2-c785-d1dca9391a6b","executionInfo":{"status":"error","timestamp":1556811997529,"user_tz":-120,"elapsed":7954,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":726}},"source":["import argparse\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import numpy as np\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.25 # dropout applied to layers (default: 0.25)\n","args.clip = 0.2 # gradient clip, -1 means no clip (default: 0.2)\n","args.epochs = 100 # upper epoch limit (default: 100)\n","args.ksize = 5 # kernel size (default: 5)\n","args.levels = 4 # # of levels (default: 4)\n","args.log_interval = 100 # report interval (default: 100)\n","args.lr = 1e-3 # initial learning rate (default: 1e-3)\n","args.optim = 'Adam' # optimizer to use (default: Adam)\n","args.nhid = 150 # number of hidden units per layer (default: 150)\n","args.data = 'JSB' #'Nott' # the dataset to run (default: Nott)\n","args.seed = 1111 # random seed (default: 1111)\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","print(args)\n","input_size = 88\n","X_train, X_valid, X_test = data_generator(args.data)\n","\n","n_channels = [args.nhid] * args.levels\n","kernel_size = args.ksize\n","dropout = args.dropout\n","\n","model = TCN(input_size, input_size, n_channels, kernel_size, dropout=args.dropout)\n","\n","\n","if args.cuda:\n","    model.cuda()\n","\n","criterion = nn.CrossEntropyLoss()\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def evaluate(X_data, name='Eval'):\n","    model.eval()\n","    eval_idx_list = np.arange(len(X_data), dtype=\"int32\")\n","    total_loss = 0.0\n","    count = 0\n","    with torch.no_grad():\n","        for idx in eval_idx_list:\n","            data_line = X_data[idx]\n","            x, y = Variable(data_line[:-1]), Variable(data_line[1:])\n","            if args.cuda:\n","                x, y = x.cuda(), y.cuda()\n","            output = model(x.unsqueeze(0)).squeeze(0)\n","            loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n","                                torch.matmul((1-y), torch.log(1-output).float().t()))\n","            total_loss += loss.item()\n","            count += output.size(0)\n","        eval_loss = total_loss / count\n","        print(name + \" loss: {:.5f}\".format(eval_loss))\n","        return eval_loss\n","\n","\n","def train(ep):\n","    model.train()\n","    total_loss = 0\n","    count = 0\n","    train_idx_list = np.arange(len(X_train), dtype=\"int32\")\n","    np.random.shuffle(train_idx_list)\n","    for idx in train_idx_list:\n","        data_line = X_train[idx]\n","        x, y = Variable(data_line[:-1]), Variable(data_line[1:])\n","        if args.cuda:\n","            x, y = x.cuda(), y.cuda()\n","\n","        optimizer.zero_grad()\n","        output = model(x.unsqueeze(0)).squeeze(0)\n","        loss = -torch.trace(torch.matmul(y, torch.log(output).float().t()) +\n","                            torch.matmul((1 - y), torch.log(1 - output).float().t()))\n","        total_loss += loss.item()\n","        count += output.size(0)\n","\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        loss.backward()\n","        optimizer.step()\n","        if idx > 0 and idx % args.log_interval == 0:\n","            cur_loss = total_loss / count\n","            print(\"Epoch {:2d} | lr {:.5f} | loss {:.5f}\".format(ep, lr, cur_loss))\n","            total_loss = 0.0\n","            count = 0\n","\n","\n","if __name__ == \"__main__\":\n","    best_vloss = 1e8\n","    vloss_list = []\n","    model_name = \"poly_music_{0}.pt\".format(args.data)\n","    for ep in range(1, args.epochs+1):\n","        train(ep)\n","        vloss = evaluate(X_valid, name='Validation')\n","        tloss = evaluate(X_test, name='Test')\n","        if vloss < best_vloss:\n","            with open(model_name, \"wb\") as f:\n","                torch.save(model, f)\n","                print(\"Saved model!\\n\")\n","            best_vloss = vloss\n","        if ep > 10 and vloss > max(vloss_list[-3:]):\n","            lr /= 10\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr\n","\n","        vloss_list.append(vloss)\n","\n","    print('-' * 89)\n","    model = torch.load(open(model_name, \"rb\"))\n","    tloss = evaluate(X_test)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7f0050097278>\n","loading JSB data...\n","Epoch  1 | lr 0.00100 | loss 16.01851\n","Epoch  1 | lr 0.00100 | loss 11.24600\n","Validation loss: 10.52090\n","Test loss: 10.62813\n","Saved model!\n","\n","Epoch  2 | lr 0.00100 | loss 10.67288\n","Epoch  2 | lr 0.00100 | loss 10.24344\n","Validation loss: 9.66332\n","Test loss: 9.76885\n","Saved model!\n","\n","Epoch  3 | lr 0.00100 | loss 9.72167\n","Epoch  3 | lr 0.00100 | loss 9.86812\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-45-1ba583d58538>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"poly_music_{0}.pt\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mvloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-45-1ba583d58538>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(ep)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"7r8wr-5FyFW2","colab_type":"code","outputId":"77e35e2b-51aa-4f4e-aefe-4228416cf0d8","executionInfo":{"status":"ok","timestamp":1556738430364,"user_tz":-120,"elapsed":598,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":130}},"source":["print(X_train.shape)\n","print(X_valid.shape)\n","print(X_test.shape)\n","\n","print(X_train[0].shape)\n","print(X_valid[0].shape)\n","print(X_test[0].shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(229,)\n","(76,)\n","(77,)\n","torch.Size([129, 88])\n","torch.Size([52, 88])\n","torch.Size([84, 88])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3eL2T7yPju8I","colab_type":"text"},"source":["# Word CNN"]},{"cell_type":"markdown","metadata":{"id":"EvS_dEX7j6L_","colab_type":"text"},"source":["In word-level language modeling tasks, each element of the sequence is a word, where the model\n","is expected to predict the next incoming word in the text. We evaluate the temporal convolutional\n","network as a word-level language model on three datasets: PennTreebank (PTB), Wikitext-103 \n","and LAMBADA.\n","\n","Because the evaluation of LAMBADA has different requirement (predicting only the very last word\n","based on a broader context), we put it in another directory. See `../lambada_language`. "]},{"cell_type":"markdown","metadata":{"id":"BA9RuCL6OoIl","colab_type":"text"},"source":["NOT WORKING! DATA NOT DOWNLOADABLE AND NOT AVAILABLE!"]},{"cell_type":"markdown","metadata":{"id":"qHB-92PVLkUj","colab_type":"text"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"QPPVltrsLkQK","colab_type":"code","colab":{}},"source":["import os\n","import torch\n","from torch.autograd import Variable\n","import pickle\n","\n","\"\"\"\n","Note: The meaning of batch_size in PTB is different from that in MNIST example. In MNIST, \n","batch_size is the # of sample data that is considered in each iteration; in PTB, however,\n","it is the number of segments to speed up computation. \n","\n","The goal of PTB is to train a language model to predict the next word.\n","\"\"\"\n","\n","\n","def data_generator(args):\n","    if os.path.exists(args.data + \"/corpus\") and not args.corpus:\n","        corpus = pickle.load(open(args.data + '/corpus', 'rb'))\n","    else:\n","        corpus = Corpus(args.data)\n","        pickle.dump(corpus, open(args.data + '/corpus', 'wb'))\n","    return corpus\n","\n","\n","class Dictionary(object):\n","    def __init__(self):\n","        self.word2idx = {}\n","        self.idx2word = []\n","\n","    def add_word(self, word):\n","        if word not in self.word2idx:\n","            self.idx2word.append(word)\n","            self.word2idx[word] = len(self.idx2word) - 1\n","        return self.word2idx[word]\n","\n","    def __len__(self):\n","        return len(self.idx2word)\n","\n","\n","class Corpus(object):\n","    def __init__(self, path):\n","        self.dictionary = Dictionary()\n","        self.train = self.tokenize(os.path.join(path, 'train.txt'))\n","        self.valid = self.tokenize(os.path.join(path, 'valid.txt'))\n","        self.test = self.tokenize(os.path.join(path, 'test.txt'))\n","\n","    def tokenize(self, path):\n","        \"\"\"Tokenizes a text file.\"\"\"\n","        assert os.path.exists(path)\n","        # Add words to the dictionary\n","        with open(path, 'r') as f:\n","            tokens = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                tokens += len(words)\n","                for word in words:\n","                    self.dictionary.add_word(word)\n","\n","        # Tokenize file content\n","        with open(path, 'r') as f:\n","            ids = torch.LongTensor(tokens)\n","            token = 0\n","            for line in f:\n","                words = line.split() + ['<eos>']\n","                for word in words:\n","                    ids[token] = self.dictionary.word2idx[word]\n","                    token += 1\n","\n","        return ids\n","\n","\n","def batchify(data, batch_size, args):\n","    \"\"\"The output should have size [L x batch_size], where L could be a long sequence length\"\"\"\n","    # Work out how cleanly we can divide the dataset into batch_size parts (i.e. continuous seqs).\n","    nbatch = data.size(0) // batch_size\n","    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n","    data = data.narrow(0, 0, nbatch * batch_size)\n","    # Evenly divide the data across the batch_size batches.\n","    data = data.view(batch_size, -1)\n","    if args.cuda:\n","        data = data.cuda()\n","    return data\n","\n","\n","def get_batch(source, i, args, seq_len=None, evaluation=False):\n","    seq_len = min(seq_len if seq_len else args.seq_len, source.size(1) - 1 - i)\n","    data = Variable(source[:, i:i+seq_len], volatile=evaluation)\n","    target = Variable(source[:, i+1:i+1+seq_len])     # CAUTION: This is un-flattened!\n","    return data, target"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_4hnl8eGLkLr","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"VJTbeUgSLkG6","colab_type":"code","colab":{}},"source":["import torch\n","from torch import nn\n","\n","\n","class TCN(nn.Module):\n","\n","    def __init__(self, input_size, output_size, num_channels,\n","                 kernel_size=2, dropout=0.3, emb_dropout=0.1, tied_weights=False):\n","        super(TCN, self).__init__()\n","        self.encoder = nn.Embedding(output_size, input_size)\n","        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size, dropout=dropout)\n","\n","        self.decoder = nn.Linear(num_channels[-1], output_size)\n","        if tied_weights:\n","            if num_channels[-1] != input_size:\n","                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n","            self.decoder.weight = self.encoder.weight\n","            print(\"Weight tied\")\n","        self.drop = nn.Dropout(emb_dropout)\n","        self.emb_dropout = emb_dropout\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        self.encoder.weight.data.normal_(0, 0.01)\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.normal_(0, 0.01)\n","\n","    def forward(self, input):\n","        \"\"\"Input ought to have dimension (N, C_in, L_in), where L_in is the seq_len; here the input is (N, L, C)\"\"\"\n","        emb = self.drop(self.encoder(input))\n","        y = self.tcn(emb.transpose(1, 2)).transpose(1, 2)\n","        y = self.decoder(y)\n","        return y.contiguous()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UERjCFAULkCR","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"tCIJvHpDLj88","colab_type":"code","outputId":"07bdc66e-5292-43d6-8cde-b57291a5ae1c","executionInfo":{"status":"error","timestamp":1556812000584,"user_tz":-120,"elapsed":805,"user":{"displayName":"Valkyrias Sairyklav","photoUrl":"https://lh6.googleusercontent.com/-egNzIrf8UQ8/AAAAAAAAAAI/AAAAAAAAAhc/8zfJAktQfIM/s64/photo.jpg","userId":"01454228790974734447"}},"colab":{"base_uri":"https://localhost:8080/","height":424}},"source":["import argparse\n","import time\n","import math\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import pickle\n","from random import randint\n","\n","\n","# Arguments.\n","args = ArgsObject()\n","args.batch_size = 16 # batch size (default: 16)\n","args.cuda = True # use CUDA (default: True)\n","args.dropout = 0.45 # dropout applied to layers (default: 0.45)\n","args.emb_dropout = 0.25 # dropout applied to the embedded layer (default: 0.25)\n","args.clip = 0.35 # gradient clip, -1 means no clip (default: 0.35)\n","args.epochs = 100 # upper epoch limit (default: 100)\n","args.ksize = 3 # kernel size (default: 3)\n","args.data = './data/penn' # location of the data corpus (default: ./data/penn)\n","args.emsize = 600 # size of word embeddings (default: 600)\n","args.levels = 4 # # of levels (default: 4)\n","args.log_interval = 100 # report interval (default: 100)\n","args.lr = 4 # initial learning rate (default: 4)\n","args.nhid = 600 # number of hidden units per layer (default: 600)\n","args.seed = 1111 # random seed (default: 1111)\n","args.tied = True # tie the encoder-decoder weights (default: True)\n","args.optim = 'SGD' # optimizer type (default: SGD)\n","args.validseqlen = 40 # valid sequence length (default: 40)\n","args.seq_len = 80 # total sequence length, including effective history (default: 80)\n","args.corpus = False # force re-make the corpus (default: False)\n","\n","# Set the random seed manually for reproducibility.\n","torch.manual_seed(args.seed)\n","if torch.cuda.is_available():\n","    if not args.cuda:\n","        print(\"WARNING: You have a CUDA device, so you should probably run with --cuda\")\n","\n","print(args)\n","corpus = data_generator(args)\n","eval_batch_size = 10\n","train_data = batchify(corpus.train, args.batch_size, args)\n","val_data = batchify(corpus.valid, eval_batch_size, args)\n","test_data = batchify(corpus.test, eval_batch_size, args)\n","\n","\n","n_words = len(corpus.dictionary)\n","\n","num_chans = [args.nhid] * (args.levels - 1) + [args.emsize]\n","k_size = args.ksize\n","dropout = args.dropout\n","emb_dropout = args.emb_dropout\n","tied = args.tied\n","model = TCN(args.emsize, n_words, num_chans, dropout=dropout, emb_dropout=emb_dropout, kernel_size=k_size, tied_weights=tied)\n","\n","if args.cuda:\n","    model.cuda()\n","\n","# May use adaptive softmax to speed up training\n","criterion = nn.CrossEntropyLoss()\n","\n","lr = args.lr\n","optimizer = getattr(optim, args.optim)(model.parameters(), lr=lr)\n","\n","\n","def evaluate(data_source):\n","    model.eval()\n","    total_loss = 0\n","    processed_data_size = 0\n","    with torch.no_grad():\n","        for i in range(0, data_source.size(1) - 1, args.validseqlen):\n","            if i + args.seq_len - args.validseqlen >= data_source.size(1) - 1:\n","                continue\n","            data, targets = get_batch(data_source, i, args, evaluation=True)\n","            output = model(data)\n","\n","            # Discard the effective history, just like in training\n","            eff_history = args.seq_len - args.validseqlen\n","            final_output = output[:, eff_history:].contiguous().view(-1, n_words)\n","            final_target = targets[:, eff_history:].contiguous().view(-1)\n","\n","            loss = criterion(final_output, final_target)\n","\n","            # Note that we don't add TAR loss here\n","            total_loss += (data.size(1) - eff_history) * loss.item()\n","            processed_data_size += data.size(1) - eff_history\n","        return total_loss / processed_data_size\n","\n","\n","def train():\n","    # Turn on training mode which enables dropout.\n","    global train_data\n","    model.train()\n","    total_loss = 0\n","    start_time = time.time()\n","    for batch_idx, i in enumerate(range(0, train_data.size(1) - 1, args.validseqlen)):\n","        if i + args.seq_len - args.validseqlen >= train_data.size(1) - 1:\n","            continue\n","        data, targets = get_batch(train_data, i, args)\n","        optimizer.zero_grad()\n","        output = model(data)\n","\n","        # Discard the effective history part\n","        eff_history = args.seq_len - args.validseqlen\n","        if eff_history < 0:\n","            raise ValueError(\"Valid sequence length must be smaller than sequence length!\")\n","        final_target = targets[:, eff_history:].contiguous().view(-1)\n","        final_output = output[:, eff_history:].contiguous().view(-1, n_words)\n","        loss = criterion(final_output, final_target)\n","\n","        loss.backward()\n","        if args.clip > 0:\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","        if batch_idx % args.log_interval == 0 and batch_idx > 0:\n","            cur_loss = total_loss / args.log_interval\n","            elapsed = time.time() - start_time\n","            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.5f} | ms/batch {:5.5f} | '\n","                  'loss {:5.2f} | ppl {:8.2f}'.format(\n","                epoch, batch_idx, train_data.size(1) // args.validseqlen, lr,\n","                elapsed * 1000 / args.log_interval, cur_loss, math.exp(cur_loss)))\n","            total_loss = 0\n","            start_time = time.time()\n","\n","\n","if __name__ == \"__main__\":\n","    best_vloss = 1e8\n","\n","    # At any point you can hit Ctrl + C to break out of training early.\n","    try:\n","        all_vloss = []\n","        for epoch in range(1, args.epochs+1):\n","            epoch_start_time = time.time()\n","            train()\n","            val_loss = evaluate(val_data)\n","            test_loss = evaluate(test_data)\n","\n","            print('-' * 89)\n","            print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n","                    'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                               val_loss, math.exp(val_loss)))\n","            print('| end of epoch {:3d} | time: {:5.2f}s | test loss {:5.2f} | '\n","                  'test ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n","                                            test_loss, math.exp(test_loss)))\n","            print('-' * 89)\n","\n","            # Save the model if the validation loss is the best we've seen so far.\n","            if val_loss < best_vloss:\n","                with open(\"model.pt\", 'wb') as f:\n","                    print('Save model!\\n')\n","                    torch.save(model, f)\n","                best_vloss = val_loss\n","\n","            # Anneal the learning rate if the validation loss plateaus\n","            if epoch > 5 and val_loss >= max(all_vloss[-5:]):\n","                lr = lr / 2.\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] = lr\n","            all_vloss.append(val_loss)\n","\n","    except KeyboardInterrupt:\n","        print('-' * 89)\n","        print('Exiting from training early')\n","\n","    # Load the best saved model.\n","    with open(\"model.pt\", 'rb') as f:\n","        model = torch.load(f)\n","\n","    # Run on test data.\n","    test_loss = evaluate(test_data)\n","    print('=' * 89)\n","    print('| End of training | test loss {:5.2f} | test ppl {:8.2f}'.format(\n","        test_loss, math.exp(test_loss)))\n","    print('=' * 89)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<__main__.ArgsObject object at 0x7f005a616cc0>\n"],"name":"stdout"},{"output_type":"error","ename":"AssertionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-2afceb5f0cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0meval_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatchify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-051b4703dc49>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/corpus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/corpus'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-051b4703dc49>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-46-051b4703dc49>\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;34m\"\"\"Tokenizes a text file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;31m# Add words to the dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAssertionError\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"XQKYQD9pXFr8","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}