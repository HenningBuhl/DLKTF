{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "KI_Praktikum_Aufgabe_2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HenningBuhl/DLKTF/blob/master/Aufgabe%202/KI_Praktikum_Aufgabe_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6P3RF01s_dW",
        "colab_type": "text"
      },
      "source": [
        "# KI Praktikum - Aufgabe 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KiVAksuiUmz",
        "colab_type": "text"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9g7QCmjiUiH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "\n",
        "TODO:\n",
        "- Implement Batch Gradient Descent\n",
        "- Implement Dropout\n",
        "- Implement Momentum Gradient Descent\n",
        "- Implement Gradient Constraint\n",
        "\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUMQ1bAFcooV",
        "colab_type": "text"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue0Q-5XMhB6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports.\n",
        "import os\n",
        "import cv2\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "!pip install googledrivedownloader\n",
        "from google_drive_downloader import GoogleDriveDownloader as gdd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bTdjZmuicptv",
        "colab_type": "text"
      },
      "source": [
        "# Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvfY8izPhB6o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork():\n",
        "    '''\n",
        "    Contains basic neural network functionality.\n",
        "    '''\n",
        "\n",
        "    def __init__(self, layer_sizes=[], epsilon=1e-6):\n",
        "        '''\n",
        "        Creates a new instance of the class.\n",
        "\n",
        "        Args:\n",
        "            layer_sizes: An array containing the number of neurons in each\n",
        "            layer.\n",
        "            epsilon: Value used to provide numerical stability.\n",
        "        \n",
        "        Returns:\n",
        "            A new instance of the class.\n",
        "        '''\n",
        "        \n",
        "        # Infer information from input parameter(s).\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.input_size = self.layer_sizes[0]\n",
        "        self.output_size = self.layer_sizes[-1]\n",
        "        self.num_layers = len(self.layer_sizes)\n",
        "        self.num_parameters = np.sum(\n",
        "            [(self.layer_sizes[i] + 1) * self.layer_sizes[i+1]\n",
        "            for i in range(self.num_layers - 1)])\n",
        "\n",
        "        # Numerical stability.\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # Initialize the network parameters.\n",
        "        self.weights = [np.random.normal( # Xavier initialization.\n",
        "            0, # Mean.\n",
        "            (1 / (self.layer_sizes[i-1] + self.layer_sizes[i]))**(1 / 2), # Std.\n",
        "            (self.layer_sizes[i-1], self.layer_sizes[i])) # Shape\n",
        "            for i in range(1, self.num_layers)]\n",
        "        self.biases = [np.zeros( # Zero\n",
        "            (self.layer_sizes[i])) # Shape.\n",
        "            for i in range(1, self.num_layers)]\n",
        "\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        '''\n",
        "        Returns the sigmoid activation function.\n",
        "\n",
        "        Args:\n",
        "            x: The input.\n",
        "        \n",
        "        Returns:\n",
        "            The sigmoid activation function.\n",
        "        '''\n",
        "\n",
        "        # Calculate the sigmoid activation function.\n",
        "        sigmoid = 1 / (1 + np.exp(-x))\n",
        "\n",
        "        # Return the sigmoid activation function.\n",
        "        return sigmoid\n",
        "\n",
        "\n",
        "    def sigmoid_prime(self, x):\n",
        "        '''\n",
        "        Returns the first derivative of the sigmoid activation function.\n",
        "\n",
        "        Args:\n",
        "            x: The input.\n",
        "        \n",
        "        Returns:\n",
        "            The first derivative of the sigmoid activation function.\n",
        "        '''\n",
        "\n",
        "        # Calculate the sigmoid activation function.\n",
        "        sigmoid = self.sigmoid(x)\n",
        "\n",
        "        # Calculate sigmoid prime.\n",
        "        sigmoid_prime = sigmoid * (1 - sigmoid)\n",
        "        \n",
        "        # Return sigmoid prime.\n",
        "        return sigmoid_prime\n",
        "\n",
        "\n",
        "    def softmax(self, x):\n",
        "        '''\n",
        "        Return the softmax activation function.\n",
        "\n",
        "        Args:\n",
        "            x: The input.\n",
        "        \n",
        "        Returns:\n",
        "            The softmax activation function.\n",
        "        '''\n",
        "\n",
        "        # Calculate the exponentials (numerically safe).\n",
        "        e_x = np.exp(x - np.max(x))\n",
        "\n",
        "        # Calculate the softmax activation function.\n",
        "        softmax = e_x / np.maximum(self.epsilon, e_x.sum())\n",
        "\n",
        "        # Return the softmax activation function.\n",
        "        return softmax\n",
        "\n",
        "\n",
        "    def cross_entropy(self, y, y_hat):\n",
        "        '''\n",
        "        Return the cross entropy loss function.\n",
        "\n",
        "        Args:\n",
        "            y: The labels.\n",
        "            y_hat: The predictions.\n",
        "        \n",
        "        Returns:\n",
        "            The cross entropy loss.\n",
        "        '''\n",
        "\n",
        "        # Clip y_hat for numerical stability.\n",
        "        y_hat = np.clip(y_hat, self.epsilon, 1 - self.epsilon)\n",
        "\n",
        "        # Calculate cross entropy loss.\n",
        "        ce_loss = -np.sum(y * np.log(y_hat))\n",
        "\n",
        "        # Return cross entropy loss.\n",
        "        return ce_loss\n",
        "\n",
        "\n",
        "    def cross_entropy_prime(self, y, y_hat):\n",
        "        '''\n",
        "        Return the first derivative of the cross entropy loss function.\n",
        "\n",
        "        Args:\n",
        "            y: The labels.\n",
        "            y_hat: The predictions.\n",
        "        \n",
        "        Returns:\n",
        "            The first derivative of the cross entropy loss.            \n",
        "        '''\n",
        "\n",
        "        # Calculate cross entrioy prime.\n",
        "        ce_prime = y_hat - y\n",
        "\n",
        "        # Return cross entropy prime.\n",
        "        return ce_prime #/ (y_hat * (1 - y_hat)) # MATHEMATICAL SHORTCUT\n",
        "\n",
        "\n",
        "    def predict(self, x, return_all=False):\n",
        "        '''\n",
        "        Return the output of the neural network.\n",
        "\n",
        "        Args:\n",
        "            x: The input.\n",
        "            return_all: If True, return pre- and post-activations and y_hat,\n",
        "            else only y_hat (default False).\n",
        "\n",
        "        Returns:\n",
        "            The output of the neural network.\n",
        "        '''\n",
        "\n",
        "        # Initialize pre- and post-activation values.\n",
        "        z = [x] # Dummy entry so a and z can be indexed with the same index.\n",
        "        a = [x] # The first layers' activations are x.\n",
        "\n",
        "        # Calculate the layers output with the output of the previous layer.\n",
        "        for i in range(1, self.num_layers):\n",
        "            z.append(a[-1] @ self.weights[i-1] + self.biases[i-1])\n",
        "            if i == self.num_layers - 1: # Last layer.\n",
        "                a.append(self.softmax(z[-1]))\n",
        "            else: # Any other layer.\n",
        "                a.append(self.sigmoid(z[-1]))\n",
        "        \n",
        "        # If return_all=True, return pre- and post-activations.\n",
        "        if return_all:\n",
        "            return z, a, a[-1]\n",
        "        else:\n",
        "            return a[-1]\n",
        "\n",
        "\n",
        "    def fit(self, x, y, learning_rate=0.1, weight_decay=0, momentum=0):\n",
        "        '''\n",
        "        Perform gradient descent.\n",
        "\n",
        "        Args:\n",
        "            x: The input.\n",
        "            y: The label.\n",
        "            learning_rate: The learning rate (default 0.1).\n",
        "            weight_decay: The weight decay coefficient (default 0).\n",
        "            momentum: The momentum term used in the deltas (default 0).\n",
        "        \n",
        "        Returns:\n",
        "            The cost.\n",
        "        '''\n",
        "\n",
        "        # Perform a feed forward.\n",
        "        z, a, y_hat = self.predict(x, return_all=True)\n",
        "\n",
        "        # Calculate the error for each layer.\n",
        "        errors = [self.cross_entropy_prime(y, y_hat)] #* self.sigmoid_prime(z[-1])] # MATHEMATICAL SHORTCUT\n",
        "        for i in reversed(range(1, self.num_layers - 1)):\n",
        "            errors.append(self.weights[i] @ errors[-1] * self.sigmoid_prime(z[i]))\n",
        "\n",
        "        # Calculate the gradients and deltas, immediately apply the deltas.\n",
        "        for i in range(self.num_layers - 1):\n",
        "            self.weights[i] -= learning_rate * np.outer(a[i], errors[-(i+1)]) + weight_decay * self.weights[i]\n",
        "            self.biases[i] -= learning_rate * errors[-(i+1)]\n",
        "\n",
        "        # Return the cost.\n",
        "        return self.cross_entropy(y, y_hat)\n",
        "\n",
        "\n",
        "    def evaluate(self, x, y):\n",
        "        '''\n",
        "        Return the loss.\n",
        "\n",
        "        Args:\n",
        "            x: The inputs.\n",
        "            y: The labels.\n",
        "        \n",
        "        Returns:\n",
        "            The loss.\n",
        "        '''\n",
        "\n",
        "        # Perform a feed forward on the input.\n",
        "        y_hat = np.array([self.predict(x_in) for x_in in x])\n",
        "\n",
        "        # Calculate the losses for each output.\n",
        "        losses = np.array([self.cross_entropy(y[i], y_hat[i]) for i in range(y_hat.shape[0])])\n",
        "\n",
        "        # Calculate the averge loss over all examples.\n",
        "        loss = np.mean(losses)\n",
        "\n",
        "        # Return the loss.\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def accuracy(self, x, y):\n",
        "        '''\n",
        "        Returns the accuracy.\n",
        "\n",
        "        Args:\n",
        "            x: The inputs.\n",
        "            y: The labels.\n",
        "        \n",
        "        Returns:\n",
        "            The accuracy.\n",
        "        '''\n",
        "\n",
        "        # Perform a feed forward on the input.\n",
        "        y_hat = np.array([self.predict(x_in) for x_in in x])\n",
        "\n",
        "        # Calculate the accuary.\n",
        "        acc = accuracy_score(np.argmax(y, axis=1), np.argmax(y_hat, axis=1))\n",
        "\n",
        "        # Return the accuracy.\n",
        "        return acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUXyrWLP4UMF",
        "colab_type": "text"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOLbvkaG4Ue-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def shuffle_data(x, y):\n",
        "    '''\n",
        "    Shuffle the arguments the same way.\n",
        "    \n",
        "    Args:\n",
        "        x: An array.\n",
        "        y: An array.\n",
        "    \n",
        "    Returns:\n",
        "        Shuffled array (same way).\n",
        "    '''\n",
        "    \n",
        "    indices = np.arange(x.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    x, y = x[indices], y[indices]\n",
        "    return x, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM-DdhsmoeyL",
        "colab_type": "text"
      },
      "source": [
        "# Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVZ-GeTPoe2N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not (all) used yet.\n",
        "use_data_augmentation = True\n",
        "iterations = 100_000\n",
        "batch_size = 1\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sePWWDe0Qkpx",
        "colab_type": "text"
      },
      "source": [
        "# Alien Symbols"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH2r58YFQknM",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXDI9arOUDHw",
        "colab_type": "text"
      },
      "source": [
        "### Fetch Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIW5E7Bx7X4u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data path.\n",
        "base_dir = \"./data/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5T5YrthlAhgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Label dictionary.\n",
        "label_dict = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftf50WTZQkjm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data\n",
        "gdd.download_file_from_google_drive(file_id='1hidaKlWV_tLnNcabiVQIzfW_8tH54gy3',\n",
        "                                    dest_path= base_dir + 'data.zip',\n",
        "                                    unzip=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndkY05TErNgk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Meta data.\n",
        "x_dim = 25\n",
        "y_dim = 25\n",
        "channels = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcJ0zQ_V31TZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x, y = [], [] # Inputs, labels.\n",
        "\n",
        "# Read data from disk.\n",
        "dirs = [os.path.join(base_dir, dir) for dir in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, dir))] # All folders.\n",
        "for target_index, (dir) in enumerate(dirs): # For all folders.\n",
        "    files = [os.path.join(dir, f) for f in os.listdir(dir)] # All files.\n",
        "    for f in files: # For all files.\n",
        "        im = cv2.imread(f, 0) # mode=0 means grayscale.\n",
        "        im = im.reshape(-1) # Flatten image.\n",
        "        x.append(im)\n",
        "        y.append(target_index)\n",
        "    label_dict[target_index] = dir.split('/')[-1]\n",
        "\n",
        "# Convert to numpy array.\n",
        "x = np.asarray(x)\n",
        "y = np.asarray(y)\n",
        "\n",
        "# labels to categorical.\n",
        "y = to_categorical(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJQG_8jS4fSP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle  data.\n",
        "x, y = shuffle_data(x, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o4C336vxHNf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'x: [min: {np.min(x)}, max: {np.max(x)}]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBorTx3eCe6i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onyqSuKTUFZZ",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Unp8467rvz1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Normalize data horizontally.\n",
        "x = scaler.fit_transform(x.T).T\n",
        "\n",
        "# Normalize data vertically.\n",
        "#x = scaler.fit_transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOm3avBNxO6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'x: [min: {np.min(x)}, max: {np.max(x)}]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzdSfQ_HwEpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_ha1hRghCOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training, validation and test set ratio.\n",
        "train_split = 0.8\n",
        "validation_split = 0.2\n",
        "\n",
        "# Calculate split indices.\n",
        "train_split_idx = int(x.shape[0] * train_split)\n",
        "validation_split_idx = int(train_split_idx * validation_split)\n",
        "\n",
        "# Create training set.\n",
        "x_train = x[:train_split_idx]\n",
        "y_train = y[:train_split_idx]\n",
        "\n",
        "# Create testing set.\n",
        "x_test = x[train_split_idx:]\n",
        "y_test = y[train_split_idx:]\n",
        "\n",
        "# Normalize data.\n",
        "#scaler.fit(x_train)\n",
        "#x_train = scaler.transform(x_train)\n",
        "#x_test = scaler.transform(x_test)\n",
        "\n",
        "# Create validation set.\n",
        "x_val = x_train[:validation_split_idx]\n",
        "y_val = y_train[:validation_split_idx]\n",
        "\n",
        "# Re-Create training set.\n",
        "x_train = x_train[validation_split_idx:]\n",
        "y_train = y_train[validation_split_idx:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2awKxf5vcZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'Train: [min: {np.min(x_train)}, max: {np.max(x_train)}]')\n",
        "print(f'Val:   [min: {np.min(x_val)}, max: {np.max(x_val)}]')\n",
        "print(f'Test:  [min: {np.min(x_test)}, max: {np.max(x_test)}]')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjX9PAVfiAa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "print(x_val.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOlJmcwqjKHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Shuffle  data.\n",
        "x_train, y_train = shuffle_data(x_train, y_train)\n",
        "x_test, y_test = shuffle_data(x_test, y_test)\n",
        "x_val, y_val = shuffle_data(x_val, y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eM5HnEKv1J-h",
        "colab_type": "text"
      },
      "source": [
        "### Visualize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQQtXDFC31Hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inspect images manually.\n",
        "index = np.random.randint(0, x_train.shape[0])\n",
        "plt.imshow(x_train[index].reshape(x_dim, y_dim), cmap='gray', clim=(0, 1))\n",
        "plt.title(f'Class: {label_dict[np.argmax(y_train[index])]}')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uujtj3gSncL8",
        "colab_type": "text"
      },
      "source": [
        "### Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAlY2ehxncHn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_datagen = ImageDataGenerator(\n",
        "    featurewise_center=False, # Default: False\n",
        "    samplewise_center=False, # Default: False\n",
        "    featurewise_std_normalization=False, # Default: False\n",
        "    samplewise_std_normalization=False, # Default: False\n",
        "    zca_whitening=False, # Default: False\n",
        "    zca_epsilon=1e-06, # Default: 1e-06\n",
        "    rotation_range=20, # Default: 0\n",
        "    width_shift_range=0.05, # Default: 0.0\n",
        "    height_shift_range=0.05, # Default: 0.0\n",
        "    brightness_range=None, # Default: None\n",
        "    shear_range=0.05, # Default: 0.0\n",
        "    zoom_range=0.05, # Default: 0.0\n",
        "    channel_shift_range=0.05, # Default: 0.0\n",
        "    fill_mode='nearest', # Default: 'nearest'\n",
        "    cval=0.0, # Default: 0.0\n",
        "    horizontal_flip=False, # Default: False\n",
        "    vertical_flip=False, # Default: False\n",
        "    rescale=None, # Default: None\n",
        "    preprocessing_function=None, # Default: None\n",
        "    data_format=None, # Default: None\n",
        "    validation_split=0.0, # Default: 0.0\n",
        "    dtype=None # Default: None\n",
        ")\n",
        "\n",
        "train_datagen.fit(\n",
        "    x=x_train.reshape(-1, x_dim, y_dim, channels),\n",
        "    augment=False, # Default: False\n",
        "    rounds=1, # Default: 1\n",
        "    seed=None # Default: None\n",
        ")\n",
        "\n",
        "train_iterator = train_datagen.flow(\n",
        "    x=x_train.reshape(-1, x_dim, y_dim, channels),\n",
        "    y=y_train, # Default: None\n",
        "    batch_size=1, # Default: 32\n",
        "    shuffle=True, # Default: True\n",
        "    sample_weight=None, # Default: None\n",
        "    seed=None, # Default: None\n",
        "    save_to_dir=None, # Default: None\n",
        "    save_prefix='', # Default: ''\n",
        "    save_format='png', # Default: 'png'\n",
        "    subset=None # Default: None\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roI6LNrPncFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_batch, y_batch = train_iterator.next()\n",
        "plt.imshow(x_batch[0].reshape(x_dim, y_dim) ,cmap='gray', clim=(0, 1))\n",
        "plt.title(f'Class: {label_dict[np.argmax(y_batch[0])]}')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UBuGnI1YVck",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-b820Kue7zT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_neurons = x_dim * y_dim\n",
        "output_neurons = len(label_dict)\n",
        "layer_sizes = [input_neurons, output_neurons]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2K6gn8jYT05",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNetwork(layer_sizes) # Adjust for perceptron vs MLP.\n",
        "print(f'Number of parameters: {nn.num_parameters}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj7zk_LVQkYv",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i1j4oPK_QkTK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dictionary to log metrics.\n",
        "history = {'train_loss' : [],\n",
        "           'train_acc' : [],\n",
        "           'val_loss' : [],\n",
        "           'val_acc' : []}\n",
        "\n",
        "# Training parameters.\n",
        "iterations = 100_000\n",
        "report_iterations = 500\n",
        "learning_rates = np.linspace(0.01, 0.0001, iterations)\n",
        "weight_decays = np.linspace(0.000001, 0.00001, iterations)\n",
        "\n",
        "# Running losses list.\n",
        "losses = []\n",
        "\n",
        "# Training loop.\n",
        "for i in range(iterations):\n",
        "    # Get training examples.\n",
        "    if use_data_augmentation:\n",
        "        x_batch, y_batch = train_iterator.next()\n",
        "        x_batch = x_batch.reshape(x_dim * y_dim)\n",
        "        y_batch = y_batch.reshape(5)\n",
        "    else:\n",
        "        index = int(np.random.uniform(0, x_train.shape[0])) # Random sample.\n",
        "        x_batch, y_batch = x_train[index,:], y_train[index]\n",
        "\n",
        "    # Train on examples.\n",
        "    loss = nn.fit(x_batch,\n",
        "                  y_batch,\n",
        "                  learning_rate=learning_rates[i],\n",
        "                  weight_decay=weight_decays[i])\n",
        "    losses.append(loss)\n",
        "\n",
        "    # Report every n iterations.\n",
        "    if not i % report_iterations:\n",
        "        # Calculate metrics.\n",
        "        train_loss = nn.evaluate(x_train, y_train)\n",
        "        train_acc = nn.accuracy(x_train, y_train)\n",
        "        val_loss = nn.evaluate(x_val, y_val)\n",
        "        val_acc = nn.accuracy(x_val, y_val)\n",
        "\n",
        "        # Debug metrics.\n",
        "        print(f'Iternation: {i:{int(np.log10(iterations))}d} ' \\\n",
        "              f'[Running loss: {np.mean(np.array(losses)):8.5f}] ' \\\n",
        "              f'[Train loss: {train_loss:8.5f}, acc: {train_acc * 100:6.2f}%] ' \\\n",
        "              f'[Val loss: {val_loss:8.5f} acc: {val_acc * 100:6.2f}%]')\n",
        "\n",
        "        # Log metrics.\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['val_acc'].append(val_acc)\n",
        "\n",
        "        # Reset running losses.\n",
        "        losses = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TueHdLtIno6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot loss.\n",
        "plt.plot(history['train_loss'], label='Train Loss')\n",
        "plt.plot(history['val_loss'], label='Val Loss')\n",
        "plt.title('Train and Val Loss')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iehESfj6Qyqf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot accuracy.\n",
        "plt.plot(history['train_acc'], label='Train Accuracy')\n",
        "plt.plot(history['val_acc'], label='Val Accuracy')\n",
        "plt.title('Train and Val Accuracy')\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O3w8fek4nerY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Final results.\n",
        "train_loss = nn.evaluate(x_train, y_train)\n",
        "train_acc = nn.accuracy(x_train, y_train)\n",
        "\n",
        "val_loss = nn.evaluate(x_val, y_val)\n",
        "val_acc = nn.accuracy(x_val, y_val)\n",
        "\n",
        "test_loss = nn.evaluate(x_test, y_test)\n",
        "test_acc = nn.accuracy(x_test, y_test)\n",
        "\n",
        "# Print final results.\n",
        "print(f'Train loss: {train_loss:8.5f}, acc: {train_acc * 100:6.2f}%')\n",
        "print(f'Val loss:   {val_loss:8.5f}, acc: {val_acc * 100:6.2f}%')\n",
        "print(f'Test loss:  {test_loss:8.5f}, acc: {test_acc * 100:6.2f}%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOMRUbRGLjmW",
        "colab_type": "text"
      },
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dxzm8wOjLjhW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmNjzmMTQrF4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG53WS02QrDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMWcxzW65qiX",
        "colab_type": "text"
      },
      "source": [
        "# Old Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54W0ZkuT5qeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4615Rjl5qbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VodlaPD35qYt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaZXMhRrPrnB",
        "colab_type": "text"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u7tqL_LPts7",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK3rJV6nPvZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dummy data.\n",
        "x = np.array([0.24, 0, 0.84, 0.46, 0.72])\n",
        "y = np.array([1, 0, 0, 0, 0, 0, 0, 0, 0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEtYyPIKXhju",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJxvNw5vXhfW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create neural network.\n",
        "nn = NeuralNetwork([5, 30, 50, 70, 9])\n",
        "print(f'Number of parameters: {nn.num_parameters}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4ddBOdtXWwR",
        "colab_type": "text"
      },
      "source": [
        "## Activations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgBQHXC8XWqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z, a, y_hat = nn.predict(x, return_all=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCFjr3UNXWnC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for z_ in z:\n",
        "    print(z_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4cbxYrmX0g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for a_ in a:\n",
        "    print(a_.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxZEYJ57X0c5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(y_hat.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUKDRYGXPwKm",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6DFXvRYhB7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test when training on a single data point.\n",
        "for i in range(10):\n",
        "    loss = nn.fit(x, y, learning_rate=0.1)\n",
        "    print(f'Epoch: {i:6d}, Loss: {loss:14.12f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cp0ISz1Poiw",
        "colab_type": "text"
      },
      "source": [
        "# MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj4-KrnUc8MQ",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIYcae--hB7Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 28 * 28)\n",
        "x_test = x_test.reshape(-1, 28 * 28)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-krP30Nfb06H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train / 255\n",
        "x_test = x_test / 255\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH9Gm-iRhB7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'x_train shape: {x_train.shape}')\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'x_test shape: {x_test.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pC5_C0JtdOlB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(f'x_train, min: {np.min(x_train)}, max: {np.max(x_train)}')\n",
        "print(f'x_test, min: {np.min(x_test)}, max: {np.max(x_test)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVCH1vipYQMk",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kA24PQKBYQE2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNetwork([28 * 28, 300, 10])\n",
        "print(f'Number of parameters: {nn.num_parameters}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6rZvXbOc-KX",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8yTaaMkhB7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "losses = []\n",
        "for i in range(100_000):\n",
        "    index = int(np.random.uniform(0, x_test.shape[0]))\n",
        "    loss = nn.fit(x_test[index,:], y_test[index], learning_rate=0.01)\n",
        "    losses.append(loss)\n",
        "    \n",
        "    if not i % 500:\n",
        "        acc = nn.accuracy(x_test, y_test)\n",
        "        print(f'Loss: {np.mean(np.array(losses)):6.4f}, Accuracy: {acc * 100:5.2f}%')\n",
        "        losses = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCGdQRt97f2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}